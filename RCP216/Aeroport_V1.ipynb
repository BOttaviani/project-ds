{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "//IMPORTER LA BIBLIOTHEQUE SQL SPARK\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//IMPORTER LA BIBLIOTHEQUE GRAPHFRAME\n",
    "import org.graphframes._\n",
    "\n",
    "//IMPORTER LA BIBLIOTHEQUE GRAPHX\n",
    "import org.apache.spark.graphx._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aeroports_struct_noeuds = StructType(StructField(Code_comp,StringType,true), StructField(num_comp,StringType,true), StructField(code_source,StringType,true), StructField(id,StringType,true), StructField(code_dest,StringType,true), StructField(num_dest,StringType,true), StructField(filler,StringType,true), StructField(nb_etapes,ShortType,true), StructField(code,StringType,true))\n",
       "aeroports_struct_liens = StructType(StructField(Code_comp,StringType,true), StructField(num_comp,StringType,true), StructField(code_source,StringType,true), StructField(src,StringType,true), StructField(code_dest,StringType,true), StructField(dst,StringType,true), StructField(filler,StringType,true), StructField(nb_etapes,ShortType,true...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(Code_comp,StringType,true), StructField(num_comp,StringType,true), StructField(code_source,StringType,true), StructField(src,StringType,true), StructField(code_dest,StringType,true), StructField(dst,StringType,true), StructField(filler,StringType,true), StructField(nb_etapes,ShortType,true), StructField(code,StringType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//DEFINITION DE LA STRUCTURE DU FICHIER D'ENTREE POUR LES NOEUDS\n",
    "val aeroports_struct_noeuds = StructType(StructField(\"Code_comp\", StringType, true) :: StructField(\"num_comp\", StringType, true) :: StructField(\"code_source\", StringType, true) :: StructField(\"id\", StringType, true) :: StructField(\"code_dest\", StringType, true) :: StructField(\"num_dest\", StringType, true) :: StructField(\"filler\", StringType, true) :: StructField(\"nb_etapes\", ShortType, true) :: StructField(\"code\", StringType, true) :: Nil)\n",
    "\n",
    "//DEFINITION DE LA STRUCTURE DU FICHIER D'ENTREE POUR LES LIENS\n",
    "val aeroports_struct_liens = StructType(StructField(\"Code_comp\", StringType, true) :: StructField(\"num_comp\", StringType, true) :: StructField(\"code_source\", StringType, true) :: StructField(\"src\", StringType, true) :: StructField(\"code_dest\", StringType, true) :: StructField(\"dst\", StringType, true) :: StructField(\"filler\", StringType, true) :: StructField(\"nb_etapes\", ShortType, true) :: StructField(\"code\", StringType, true) :: Nil)\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE A L'AIDE DE LA STRUCTURE DES NOEUDS DEFINIE PLUS HAUT\n",
    "val aeroports_noeuds = spark.read.format(\"csv\").schema(aeroports_struct_noeuds).load(\"routes.dat\")\n",
    "\n",
    "//CREATION DE LA TABLE ASSOCIEE AUX NOEUDS\n",
    "aeroports_noeuds.createOrReplaceTempView(\"aeroports_noeuds\")\n",
    "\n",
    "//SELECTION DES DONNEES SERVANT A LA CONSTRUCTION DU GRAPHE A PARTIR DE LA TABLE DES NOEUDS\n",
    "val Noeuds = spark.sql(\"select distinct id, code_source from (select id, code_source from aeroports_noeuds where id not like '%N' and code_source not like '%N' union select num_dest, code_dest from aeroports_noeuds)\")\n",
    "Noeuds.createOrReplaceTempView(\"Noeuds\")\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE A L'AIDE DE LA STRUCTURE DES LIENS DEFINIE PLUS HAUT\n",
    "val aeroports_liens = spark.read.format(\"csv\").schema(aeroports_struct_liens).load(\"routes.dat\")\n",
    "\n",
    "//CREATION DE LA TABLE ASSOCIEE AUX LIENS\n",
    "aeroports_liens.createOrReplaceTempView(\"aeroports_liens\")\n",
    "\n",
    "//SELECTION DES DONNEES SERVANT A LA CONSTRUCTION DU GRAPHE A PARTIR DE LA TABLE DES LIENS\n",
    "val Liens = spark.sql(\"select distinct src, code_source, dst, code_dest from aeroports_liens where src not like '%N' and dst not like '%N'\")\n",
    "Liens.createOrReplaceTempView(\"Liens\")\n",
    "\n",
    "//CONSTRUCTION DU GRAPHE\n",
    "val Graphe_aeroports = GraphFrame(Noeuds, Liens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================================================>    (185 + 4) / 200]+-------+------------------+\n",
      "|summary|             poids|\n",
      "+-------+------------------+\n",
      "|  count|             19080|\n",
      "|   mean|3.5241090146750524|\n",
      "| stddev| 2.566571683526507|\n",
      "|    min|                 1|\n",
      "|    max|                39|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Poids_Liens = [aeroport_1: string, aeroport_2: string ... 1 more field]\n",
       "Poids = [poids: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[poids: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//POIDS DES DIFFERENTES LIGNES EN FONCTION DU NOMBRE DE COMPAGNIES EXPLOITANTES (INDEPENDAMMENT DU SENS DU PARCOURS)\n",
    "val Poids_Liens = spark.sql(\"select noeud_1 as aeroport_1, noeud_2 as aeroport_2, count(*) as poids from (select substr(code,1,3) as noeud_1, substr(code,4,3) as noeud_2 from (select case when code_source < code_dest then concat(code_source,code_dest) else concat(code_dest,code_source) end as code from aeroports_liens where src not like '%N' and dst not like '%N' order by code)) group by aeroport_1, aeroport_2 order by poids desc, aeroport_1 asc\")\n",
    "Poids_Liens.createOrReplaceTempView(\"Poids_Liens\")\n",
    "val Poids = spark.sql(\"select poids from Poids_Liens\")\n",
    "Poids.describe().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_noeuds = [nb_noeuds: decimal(7,2)]\n",
       "nb_liens = [nb_liens: decimal(7,2)]\n",
       "table_densite = [nb_noeuds: decimal(7,2), nb_liens: decimal(7,2)]\n",
       "densite = [densite: decimal(5,4)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[densite: decimal(5,4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_noeuds = spark.sql(\"select cast(count(*) as decimal(7,2)) as nb_noeuds from Noeuds\")\n",
    "val nb_liens = spark.sql(\"select cast(count(*) as decimal(7,2)) as nb_liens from Poids_Liens\")\n",
    "val table_densite = nb_noeuds.crossJoin(nb_liens)\n",
    "table_densite.createOrReplaceTempView(\"table_densite\")\n",
    "val densite = spark.sql(\"select cast((2*nb_liens)/(nb_noeuds*(nb_noeuds-1)) as decimal (5,4)) as densite from table_densite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 28:===================================================>  (189 + 4) / 200]+----+---------+-----------+\n",
      "|  id|code_IATA|degre_total|\n",
      "+----+---------+-----------+\n",
      "| 340|      FRA|        477|\n",
      "|1382|      CDG|        470|\n",
      "| 580|      AMS|        463|\n",
      "|1701|      IST|        453|\n",
      "|3682|      ATL|        433|\n",
      "+----+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Stage 31:=================================================>    (183 + 4) / 200]+-------+------------------+\n",
      "|summary|            degree|\n",
      "+-------+------------------+\n",
      "|  count|              3330|\n",
      "|   mean|22.386786786786786|\n",
      "| stddev| 48.81451894696512|\n",
      "|    min|                 1|\n",
      "|    max|               477|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id: string, degree: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "degres = [id: string, degree: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//DISTRIBUTIONS DES DEGRES (PLUS PROCHES VOISINS)\n",
    "val degres = Graphe_aeroports.degrees\n",
    "degres.createOrReplaceTempView(\"degres\")\n",
    "spark.sql(\"select a.id, b.code_source as code_IATA, a.degree as degre_total from degres a inner join Noeuds b on b.id = a.id order by degree desc\").show(5)\n",
    "spark.sql(\"select degree from degres\").describe().show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 58:====================================================> (195 + 4) / 200]+-----+----+-----------+\n",
      "|count|  id|code_source|\n",
      "+-----+----+-----------+\n",
      "| 4543| 580|        AMS|\n",
      "| 4357| 340|        FRA|\n",
      "| 4136|1382|        CDG|\n",
      "| 3658| 346|        MUC|\n",
      "| 3154| 507|        LHR|\n",
      "| 3148|1701|        IST|\n",
      "| 3142|1555|        FCO|\n",
      "| 3094|1218|        BCN|\n",
      "| 2755|1229|        MAD|\n",
      "| 2665|3682|        ATL|\n",
      "| 2652|1678|        ZRH|\n",
      "| 2649| 302|        BRU|\n",
      "| 2645| 599|        DUB|\n",
      "| 2644|3830|        ORD|\n",
      "| 2601|3364|        PEK|\n",
      "| 2569|3797|        JFK|\n",
      "| 2414|1613|        VIE|\n",
      "| 2405|2188|        DXB|\n",
      "| 2387|3494|        EWR|\n",
      "| 2351| 345|        DUS|\n",
      "+-----+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[Stage 81:====================================================> (195 + 4) / 200]+---------------+\n",
      "|total_triangles|\n",
      "+---------------+\n",
      "|         302511|\n",
      "+---------------+\n",
      "\n",
      "[Stage 105:==================================================>  (192 + 4) / 200]+-------+------------------+\n",
      "|summary|             count|\n",
      "+-------+------------------+\n",
      "|  count|              3425|\n",
      "|   mean|  88.3243795620438|\n",
      "| stddev|333.25217107350767|\n",
      "|    min|                 0|\n",
      "|    max|              4543|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "triangles = [count: bigint, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[count: bigint, id: string ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CLUSTERING\n",
    "//Détermination/distribution du nombre de triangles\n",
    "val triangles = Graphe_aeroports.triangleCount.run\n",
    "triangles.createOrReplaceTempView(\"triangles\")\n",
    "spark.sql(\"select * from triangles order by count desc\").show\n",
    "spark.sql(\"select sum(count) as total_triangles from triangles\").show\n",
    "spark.sql(\"select count from triangles\").describe().show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 131:====================================================>(198 + 2) / 200]+----+-----------+------------+---------+----------+\n",
      "|  id|code_source|nb_triangles|nb_degres|clustering|\n",
      "+----+-----------+------------+---------+----------+\n",
      "|1218|        BCN|        3094|      326|   0.05840|\n",
      "| 507|        LHR|        3154|      340|   0.05473|\n",
      "| 346|        MUC|        3658|      380|   0.05080|\n",
      "| 580|        AMS|        4543|      463|   0.04248|\n",
      "| 340|        FRA|        4357|      477|   0.03838|\n",
      "|1382|        CDG|        4136|      470|   0.03753|\n",
      "|1701|        IST|        3148|      453|   0.03075|\n",
      "+----+-----------+------------+---------+----------+\n",
      "\n",
      "[Stage 156:===================================================> (194 + 4) / 200]+-------+------------------+\n",
      "|summary|        clustering|\n",
      "+-------+------------------+\n",
      "|  count|              3308|\n",
      "|   mean|       0.136940641|\n",
      "| stddev|0.1754712410326138|\n",
      "|    min|           0.00000|\n",
      "|    max|           1.00000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clustering = [id: string, code_source: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, code_source: string ... 3 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Clustering détaillé\n",
    "val clustering = spark.sql(\"select  t.id, t.code_source, t.count as nb_triangles, d.degree as nb_degres, cast((2*t.count)/(d.degree*(d.degree - 1)) as decimal(7,5) ) as clustering from triangles t inner join degres d on d.id = t.id\")\n",
    "clustering.createOrReplaceTempView(\"clustering\")\n",
    "spark.sql(\"select * from clustering where nb_degres > 318 and nb_triangles > 3029 order by clustering desc\").show\n",
    "spark.sql(\"select clustering from clustering\").describe().show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 3438:==================================================> (196 + 4) / 200]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"select id,clustering from clustering order by clustering desc\").coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(\"clustering.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 185:===================================================> (195 + 5) / 200]+-----------------+\n",
      "|clustering_global|\n",
      "+-----------------+\n",
      "|           0.1323|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somme_clustering = [somme_clustering: decimal(7,2)]\n",
       "nb_noeuds = [nb_noeuds: decimal(7,2)]\n",
       "table_clustering_global = [somme_clustering: decimal(7,2), nb_noeuds: decimal(7,2)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[somme_clustering: decimal(7,2), nb_noeuds: decimal(7,2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Clustering global\n",
    "val somme_clustering = spark.sql(\"select cast(sum(clustering) as decimal(7,2)) as somme_clustering from clustering\")\n",
    "val nb_noeuds = spark.sql(\"select cast(count(*) as decimal(7,2)) as nb_noeuds from Noeuds\")\n",
    "val table_clustering_global = somme_clustering.crossJoin(nb_noeuds)\n",
    "table_clustering_global.createOrReplaceTempView(\"table_clustering_global\")\n",
    "spark.sql(\"select cast(somme_clustering/nb_noeuds as decimal(5,4)) as clustering_global from table_clustering_global\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,3304)                                                                        \n",
      "(300647710722,10)\n",
      "(266287972363,4)\n",
      "(9,4)\n",
      "(352187318278,4)\n",
      "(523986010129,2)\n",
      "(438086664202,2)\n",
      "(326417514507,1)\n",
      "(206158430216,1)\n",
      "(687194767370,1)\n",
      "(601295421446,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GrapheX_aeroports = org.apache.spark.graphx.impl.GraphImpl@439972eb\n",
       "composants_connexes = org.apache.spark.graphx.impl.GraphImpl@77af378c\n",
       "componentCounts = ArrayBuffer((0,3304), (300647710722,10), (266287972363,4), (9,4), (352187318278,4), (523986010129,2), (438086664202,2), (326417514507,1), (206158430216,1), (687194767370,1), (601295421446,1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sortedConnectedComponents: (connectedComponents: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId, _])Seq[(org.apache.spark.graphx.VertexId, Long)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ArrayBuffer((0,3304), (300647710722,10), (266287972363,4), (9,4), (352187318278,4), (523986010129,2), (438086664202,2), (326417514507,1), (206158430216,1), (687194767370,1), (601295421446,1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//COMPOSANTES CONNEXES\n",
    "val GrapheX_aeroports = Graphe_aeroports.toGraphX\n",
    "val composants_connexes = GrapheX_aeroports.connectedComponents\n",
    "def sortedConnectedComponents(connectedComponents: Graph[VertexId, _]): Seq[(VertexId, Long)] = {\n",
    "  val componentCounts = connectedComponents.vertices.map(_._2).countByValue\n",
    "  componentCounts.toSeq.sortBy(_._2).reverse\n",
    "}\n",
    "val componentCounts = sortedConnectedComponents( composants_connexes)\n",
    "componentCounts.size\n",
    "componentCounts.take(11).foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|nb_noeuds_connectes|\n",
      "+-------------------+\n",
      "|               3304|\n",
      "+-------------------+\n",
      "\n",
      "[Stage 426:============================================>        (169 + 4) / 200]+----------------------------+\n",
      "|pourcentage_noeuds_connectes|\n",
      "+----------------------------+\n",
      "|                       96.47|\n",
      "+----------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "connexion_principale = [_1: bigint, _2: bigint]\n",
       "nb_noeuds_connectes = [nb_noeuds_connectes: bigint]\n",
       "pourcentage_noeuds_connexion_principale = [nb_noeuds_connectes: bigint, nb_noeuds: decimal(7,2)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[nb_noeuds_connectes: bigint, nb_noeuds: decimal(7,2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val connexion_principale = componentCounts.take(1).toList.toDF\n",
    "connexion_principale.createOrReplaceTempView(\"connexion_principale\")\n",
    "val nb_noeuds_connectes = spark.sql(\"select _2 as nb_noeuds_connectes from connexion_principale\")\n",
    "nb_noeuds_connectes.show\n",
    "val pourcentage_noeuds_connexion_principale = nb_noeuds_connectes.crossJoin(nb_noeuds)\n",
    "pourcentage_noeuds_connexion_principale.createOrReplaceTempView(\"pourcentage_noeuds_connexion_principale\")\n",
    "spark.sql(\"select cast((nb_noeuds_connectes/nb_noeuds)*100 as decimal(4,2)) as pourcentage_noeuds_connectes from pourcentage_noeuds_connexion_principale\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 436:==================================================>  (192 + 4) / 200]+----------+----------+----------+\n",
      "|         a|         b|         c|\n",
      "+----------+----------+----------+\n",
      "|[2397,MNL]|[2426,MPH]|[2397,MNL]|\n",
      "|[2397,MNL]|[2426,MPH]|[4206,CEB]|\n",
      "| [346,MUC]|[1558,EBA]| [382,FDH]|\n",
      "| [346,MUC]|[1558,EBA]|[1676,BRN]|\n",
      "| [346,MUC]|[1558,EBA]|[1679,ACH]|\n",
      "| [346,MUC]|[1558,EBA]|[1678,ZRH]|\n",
      "| [346,MUC]|[1558,EBA]| [346,MUC]|\n",
      "|[3682,ATL]|[5756,MSL]|[3682,ATL]|\n",
      "|[4108,TNA]|[3382,KMG]|[6347,SJW]|\n",
      "|[4108,TNA]|[3382,KMG]|[6361,YIH]|\n",
      "|[4108,TNA]|[3382,KMG]|[6944,JNG]|\n",
      "|[4108,TNA]|[3382,KMG]|[3179,HKT]|\n",
      "|[4108,TNA]|[3382,KMG]|[3043,CCU]|\n",
      "|[4108,TNA]|[3382,KMG]|[3386,HGH]|\n",
      "|[4108,TNA]|[3382,KMG]|[3304,KUL]|\n",
      "|[4108,TNA]|[3382,KMG]|[6399,LZO]|\n",
      "|[4108,TNA]|[3382,KMG]|[3370,CAN]|\n",
      "|[4108,TNA]|[3382,KMG]|[4149,XUZ]|\n",
      "|[4108,TNA]|[3382,KMG]|[3388,NKG]|\n",
      "|[4108,TNA]|[3382,KMG]|[3376,WUH]|\n",
      "+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+----------+                                                          \n",
      "|code_IATA|centralite|\n",
      "+---------+----------+\n",
      "|      FRA|     56882|\n",
      "|      CDG|     55221|\n",
      "|      AMS|     53592|\n",
      "|      IST|     51300|\n",
      "|      ATL|     46872|\n",
      "+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Stage 464:===================================================>   (70 + 5) / 75]+-------+------------------+\n",
      "|summary|        centralite|\n",
      "+-------+------------------+\n",
      "|  count|              3306|\n",
      "|   mean| 725.9258923169994|\n",
      "| stddev|3592.1085454609047|\n",
      "|    min|                 1|\n",
      "|    max|             56882|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trajets = [a: struct<id: string, code_source: string>, b: struct<id: string, code_source: string> ... 1 more field]\n",
       "intermediaires = [code_source: string]\n",
       "centralites = [code_IATA: string, centralite: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[code_IATA: string, centralite: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CENTRALITES - RECHERCHE DE PLUS COURTS TRAJETS AVEC AEROPORTS INTERMEDIAIRES\n",
    "val trajets = Graphe_aeroports.find(\"(a)-[]->(b); (b)-[]->(c)\")\n",
    "trajets.show()\n",
    "trajets.createOrReplaceTempView(\"trajets\")\n",
    "//Récupération des aéroports intermédiaires sur le plus court trajet\n",
    "val intermediaires = spark.sql(\"select b.code_source from trajets where a<>b and b<>c\")\n",
    "intermediaires.createOrReplaceTempView(\"intermediaires\")\n",
    "val centralites = spark.sql(\"select code_source as code_IATA, count(*) as centralite from intermediaires group by code_source order by centralite desc\")\n",
    "centralites.createOrReplaceTempView(\"centralites\")\n",
    "centralites.show(5)\n",
    "spark.sql(\"select centralite from centralites\").describe().show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 3489:===================================================>(199 + 1) / 200]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centralites.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(\"centralite.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 605:=================================================>   (188 + 4) / 200]+----+-------------+\n",
      "|  id|        label|\n",
      "+----+-------------+\n",
      "| 468| 549755813898|\n",
      "|1972| 695784701963|\n",
      "|5575| 549755813898|\n",
      "|1909| 549755813898|\n",
      "|8314|1030792151047|\n",
      "|2688|1374389534720|\n",
      "|8259|1494648619016|\n",
      "|2256| 420906795015|\n",
      "|1925|1030792151047|\n",
      "|6157| 549755813898|\n",
      "|6135|1030792151047|\n",
      "|3514|1030792151047|\n",
      "|5492|           16|\n",
      "|2001|1056561954834|\n",
      "|2517| 395136991234|\n",
      "|7150|1125281431570|\n",
      "|3782|1030792151047|\n",
      "|6265|1185410973711|\n",
      "|6992|1030792151047|\n",
      "|2917|  51539607564|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LPA = [id: string, code_source: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, code_source: string ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LPA (LABEL PROPAGATION) - DETERMINATION DE L'EXISTENCE DE COMMUNAUTES\n",
    "val LPA = Graphe_aeroports.labelPropagation.maxIter(5).run()\n",
    "LPA.createOrReplaceTempView(\"LPA\")\n",
    "LPA.select(\"id\", \"label\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+                                                    \n",
      "|summary|              nbre|\n",
      "+-------+------------------+\n",
      "|  count|               339|\n",
      "|   mean|10.103244837758112|\n",
      "| stddev| 56.40873097551725|\n",
      "|    min|                 1|\n",
      "|    max|               736|\n",
      "+-------+------------------+\n",
      "\n",
      "[Stage 717:===================================================> (195 + 5) / 200]+-------------+----+\n",
      "|        label|nbre|\n",
      "+-------------+----+\n",
      "| 549755813898| 736|\n",
      "|1030792151047| 675|\n",
      "| 790273982469| 211|\n",
      "| 206158430212| 123|\n",
      "|  51539607564| 110|\n",
      "| 326417514507|  92|\n",
      "| 171798691842|  85|\n",
      "|1374389534733|  81|\n",
      "| 420906795015|  77|\n",
      "| 317827579911|  28|\n",
      "|1151051235329|  28|\n",
      "|1400159338500|  27|\n",
      "| 498216206341|  25|\n",
      "| 498216206337|  23|\n",
      "| 584115552258|  23|\n",
      "| 420906795025|  23|\n",
      "| 257698037777|  22|\n",
      "|  25769803782|  22|\n",
      "| 558345748488|  21|\n",
      "| 395136991234|  21|\n",
      "+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "communautes = [label: bigint, nbre: bigint]\n",
       "LPA_stat = [nbre: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[nbre: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Les 20 groupes les plus importants\n",
    "val communautes = spark.sql(\"select label, count(*) as nbre from LPA group by label order by nbre desc\")\n",
    "communautes.createOrReplaceTempView(\"communautes\")\n",
    "val LPA_stat = spark.sql(\"select nbre from communautes\")\n",
    "LPA_stat.describe().show\n",
    "communautes.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 770:====================================================>(198 + 2) / 200]+----+-----------+------------+---------+\n",
      "|  id|code_source|nb_triangles|nb_degres|\n",
      "+----+-----------+------------+---------+\n",
      "|2975|        SVX|         416|      130|\n",
      "|2983|        TAS|         442|      113|\n",
      "|4078|        OVB|         355|      103|\n",
      "|2908|        ALA|         275|       90|\n",
      "|2910|        TSE|         169|       68|\n",
      "|2979|        DYU|         142|       61|\n",
      "|4374|        KJA|         153|       57|\n",
      "|2937|        IKT|          89|       52|\n",
      "|2912|        FRU|         118|       44|\n",
      "|2990|        KZN|          90|       42|\n",
      "|2927|        KHV|          78|       42|\n",
      "|2923|        YKS|          60|       40|\n",
      "|2992|        UFA|          74|       38|\n",
      "|6147|        LBD|          63|       38|\n",
      "|4111|        TJM|          53|       37|\n",
      "|2965|        AER|          79|       35|\n",
      "|4367|        SCO|          46|       34|\n",
      "|2934|        VVO|          49|       30|\n",
      "|4274|        GOJ|          48|       30|\n",
      "|2958|        OMS|          47|       30|\n",
      "+----+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[Stage 825:===================================================> (194 + 4) / 200]+---------+\n",
      "|sum(nbre)|\n",
      "+---------+\n",
      "|     3425|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct l.id, a.code_source, c.nb_triangles, c.nb_degres from LPA l inner join aeroports_noeuds a on a.id = l.id inner join clustering c on c.id = l.id where label = 51539607564 order by c.nb_degres desc, c.nb_triangles desc \").show\n",
    "//549755813898 (736)  => Europe\n",
    "//1030792151047 (675) => Amérique du Nord\n",
    "//790273982469 (211)  => Chine\n",
    "//206158430212 (123)  => Moyen Orient - Inde - Pakistan\n",
    "//51539607564 (110)   => Asie du Nord (Russie et anciennes républiques de l'Union soviétique) \n",
    "//420906795015 (77)   => Extrême Orient (Viêt-Nam)\n",
    "//Le nombre total de labels est égal au nombre de noeuds du réseau\n",
    "spark.sql(\"select sum(nbre) from(select label, count(*) as nbre from LPA group by label order by nbre desc)\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 873:===================================================> (194 + 4) / 200]+----------+\n",
      "|modularite|\n",
      "+----------+\n",
      "|    0.5806|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "table_Q1 = [src: string, code_src: string ... 6 more fields]\n",
       "nb_tot_liens = [nb_tot_liens: decimal(6,1)]\n",
       "table_Q2 = [src: string, code_src: string ... 7 more fields]\n",
       "table_Q3bis = [lab_src: bigint, lab_dst: bigint ... 3 more fields]\n",
       "somme_produit = [somme_produit: decimal(7,2)]\n",
       "table_Q4 = [somme_produit: decimal(7,2), nb_tot_liens: decimal(6,1)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[somme_produit: decimal(7,2), nb_tot_liens: decimal(6,1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//TABLES POUR CALCUL MODULARITE\n",
    "val table_Q1 = spark.sql(\"select a.src, b.code_source as code_src, d.degree as deg_src, f.label as lab_src, a.dst, c.code_source as code_dst, e.degree as deg_dst, g.label as lab_dst from Liens a inner join Noeuds b on b.id = a.src inner join Noeuds c on c.id = a.dst inner join degres d on d.id = a.src inner join degres e on e.id = a.dst inner join LPA f on f.id = a.src inner join LPA g on g.id = a.dst\")\n",
    "table_Q1.createOrReplaceTempView(\"table_Q1\")\n",
    "val nb_tot_liens = spark.sql(\"select cast(count(*) as decimal(6,1)) as nb_tot_liens from Poids_Liens\")\n",
    "val table_Q2 = table_Q1.crossJoin(nb_tot_liens)\n",
    "table_Q2.createOrReplaceTempView(\"table_Q2\")\n",
    "val table_Q3bis = spark.sql(\"select lab_src, lab_dst, deg_src, deg_dst, cast((deg_src*deg_dst)/(2*nb_tot_liens) as decimal (5,2)) as produit from table_Q2\")\n",
    "table_Q3bis.createOrReplaceTempView(\"table_Q3bis\")\n",
    "val somme_produit = spark.sql(\"select cast(sum(produit) as decimal(7,2)) as somme_produit from table_Q3bis\")\n",
    "somme_produit.createOrReplaceTempView(\"somme_produit\")\n",
    "val table_Q4 = somme_produit.crossJoin(nb_tot_liens)\n",
    "table_Q4.createOrReplaceTempView(\"table_Q4\")\n",
    "spark.sql(\"select cast(1-((somme_produit)/(2*nb_tot_liens)) as decimal (5,4)) as modularite from table_Q4\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1130:==================================================> (196 + 4) / 200]+----+--------------------+\n",
      "|  id|           distances|\n",
      "+----+--------------------+\n",
      "| 468|Map(340 -> 2, 555...|\n",
      "|1972|Map(340 -> 3, 555...|\n",
      "|5575|Map(340 -> 3, 555...|\n",
      "|1909|Map(340 -> 1, 555...|\n",
      "|8314|Map(340 -> 2, 555...|\n",
      "|2688|Map(340 -> 2, 555...|\n",
      "|8259|               Map()|\n",
      "|2256|Map(340 -> 2, 555...|\n",
      "|1925|Map(340 -> 3, 555...|\n",
      "|6157|Map(340 -> 2, 555...|\n",
      "|6135|Map(340 -> 3, 555...|\n",
      "|3514|Map(340 -> 2, 555...|\n",
      "|5492|Map(340 -> 4, 555...|\n",
      "|2001|               Map()|\n",
      "|2517|Map(340 -> 3, 555...|\n",
      "|7150|Map(340 -> 4, 555...|\n",
      "|3782|Map(340 -> 2, 555...|\n",
      "|6265|Map(340 -> 4, 555...|\n",
      "|6992|Map(340 -> 2, 555...|\n",
      "|2917|Map(340 -> 2, 555...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a = 340\n",
       "b = 5557\n",
       "plus_court_chemin = [id: string, code_source: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, code_source: string ... 1 more field]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Plus court chemin entre deux aéroports\n",
    "val a : String = \"340\"\n",
    "val b : String = \"5557\" \n",
    "val plus_court_chemin = Graphe_aeroports.shortestPaths.landmarks(Seq(a, b)).run()\n",
    "plus_court_chemin.select(\"id\", \"distances\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+                                                                \n",
      "|  id|component|\n",
      "+----+---------+\n",
      "|6733|        0|\n",
      "|1579|        0|\n",
      "|5575|        0|\n",
      "| 468|        0|\n",
      "|5475|        0|\n",
      "|2688|        0|\n",
      "|2256|        0|\n",
      "|1925|        0|\n",
      "|1972|        0|\n",
      "|8314|        0|\n",
      "|3514|        0|\n",
      "|5492|        0|\n",
      "|2517|        0|\n",
      "|7150|        0|\n",
      "|3782|        0|\n",
      "|6265|        0|\n",
      "|6992|        0|\n",
      "|6157|        0|\n",
      "|1909|        0|\n",
      "|6135|        0|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SC = [id: string, code_source: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, code_source: string ... 1 more field]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Strongly connected\n",
    "val SC = Graphe_aeroports.stronglyConnectedComponents.maxIter(10).run()\n",
    "SC.select(\"id\", \"component\").orderBy(\"component\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
