{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNAM UASB03 - CERTIFICATION ANALYSE DE DONNEES MASSIVES\n",
    "## Projet d'analyse de sentiment sur les commentaires Airbnb en français\n",
    "\n",
    "***\n",
    "Notebook Scala de vectorisation de texte sur l'échantillon précedemment constitué.\n",
    "\n",
    "4 vectorisations des commentaires vont être réalisées :\n",
    "- HashingTF sur les commentaires lemmatisés et nettoyés d'une liste de StopWords\n",
    "- Word2Vec à partir d'un modèle calculé sur une extraction wikipédia à fin Juin enrichi du corpus des commentaires AirBnb en langue française puis appliqué sur les commentaires bruts et nettoyés d'une liste de StopWords\n",
    "- Word2Vec à partir d'un corpus constitué des commentaires AirBnb en langue française\n",
    "- CountVectorizer sur les commentaires lemmatisés et nettoyés d'une liste de StopWords\n",
    "\n",
    "NB : la lemmatisation a été réalisé par un autre notebook en python pour utiliser la librairie __Treetager__ qui est plus adaptée que sur de la lemmatisation en français que la solution vu en TP RCP216\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Suppression des stopWords du fichier lemmatisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES\n",
    "val echantillon = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_lemmatise.csv\")\n",
    "echantillon.createOrReplaceTempView(\"echantillon\")\n",
    "val echantillonDF = spark.sql(\"select cast(Id_comment as Int) as Id, concat(qualite, '$ ',commentaire) as commentaire_enrichi from echantillon\")\n",
    "\n",
    "//DECOUPAGE DU CORPUS DE COMMENTAIRES EN MOTS\n",
    "//Définition des paramètres du découpage des commentaires en mots\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"commentaire_enrichi\")\n",
    "  .setOutputCol(\"mots\")\n",
    "//Découpage de chaque commentaire en mots\n",
    "val tokenized_df = tokenizer.transform(echantillonDF)\n",
    "\n",
    "//FILTRAGE DES STOPS WORDS\n",
    "//Lecture du fichier de stop words\n",
    "val stopwords = sc.textFile(\"Data/French_stop_words\").collect()\n",
    "//Définition des paramètres du filtre à stop words\n",
    "val remover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords) // This parameter is optional\n",
    "  .setInputCol(\"mots\")\n",
    "  .setOutputCol(\"mots_filtres\")\n",
    "//Création d'un dataframe des mots filtrés\n",
    "val filtrage_df = remover.transform(tokenized_df)\n",
    "//Création d'un dataframe avec les mots filtres uniqument\n",
    "val echantillon_filtreWArray = filtrage_df.select(\"mots_filtres\")\n",
    "\n",
    "//Remise au format String de la zone \"mots_filtres\"\n",
    "val echantillon_filtreDF = echantillon_filtreWArray.as[Array[String]]\n",
    "  .map { case (echantillon_filtreWArray) => (echantillon_filtreWArray.mkString(\" \")) }\n",
    "  .toDF(\"Liste_mots\")\n",
    "\n",
    "echantillon_filtreDF.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(\"Data/echantillon_lemmatise_SWR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Suppression des StopWords du fichier non lemmatisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES\n",
    "val echantillon = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_evalue.csv\")\n",
    "echantillon.createOrReplaceTempView(\"echantillon\")\n",
    "val echantillonDF = spark.sql(\"select cast(Id_comment as Int) as Id, concat(qualite, '$ ',commentaire) as commentaire_enrichi from echantillon\")\n",
    "\n",
    "//DECOUPAGE DU CORPUS DE COMMENTAIRES EN MOTS\n",
    "//Définition des paramètres du découpage des commentaires en mots\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"commentaire_enrichi\")\n",
    "  .setOutputCol(\"mots\")\n",
    "//Découpage de chaque commentaire en mots\n",
    "val tokenized_df = tokenizer.transform(echantillonDF)\n",
    "\n",
    "//FILTRAGE DES STOPS WORDS\n",
    "//Lecture du fichier de stop words\n",
    "val stopwords = sc.textFile(\"Data/French_stop_words\").collect()\n",
    "//Définition des paramètres du filtre à stop words\n",
    "val remover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords) // This parameter is optional\n",
    "  .setInputCol(\"mots\")\n",
    "  .setOutputCol(\"mots_filtres\")\n",
    "//Création d'un dataframe des mots filtrés\n",
    "val filtrage_df = remover.transform(tokenized_df)\n",
    "//Création d'un dataframe avec les mots filtres uniqument\n",
    "val echantillon_filtreWArray = filtrage_df.select(\"mots_filtres\")\n",
    "\n",
    "//Remise au format String de la zone \"mots_filtres\"\n",
    "val echantillon_filtreDF = echantillon_filtreWArray.as[Array[String]]\n",
    "  .map { case (echantillon_filtreWArray) => (echantillon_filtreWArray.mkString(\" \")) }\n",
    "  .toDF(\"Liste_mots\")\n",
    "\n",
    "echantillon_filtreDF.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(\"Data/echantillon_brut_SWR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Vectorisation HashingTF du fichier lemmatisé après suppression des StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_filtreDF = [Liste_mots: string]\n",
       "echantillonRDD = MapPartitionsRDD[41] at rdd at <console>:56\n",
       "hashingTF = org.apache.spark.mllib.feature.HashingTF@7f240cd0\n",
       "echantillon_reformat = MapPartitionsRDD[42] at map at <console>:59\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "echantillon...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[42] at map at <console>:59"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//VECTORISATION DES DONNEES AVEC HASHING TF\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES\n",
    "val echantillon_filtreDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_lemmatise_SWR\")\n",
    "\n",
    "//CONSTRUCTION DU FORMAT LIBSVM\n",
    "//Variables d'entrée\n",
    "val echantillonRDD = echantillon_filtreDF.rdd\n",
    "val hashingTF = new HashingTF(12000)\n",
    "//Constitution du message avec le label\n",
    "val echantillon_reformat = echantillonRDD.map(\n",
    "  row =>{\n",
    "    Try{\n",
    "      val msg = row.toString.toLowerCase()\n",
    "      var isHappy:Int = 0\n",
    "      if(msg.contains(\"positif$\")){\n",
    "        isHappy = 1\n",
    "      }else if(msg.contains(\"negatif$\")){\n",
    "        isHappy = 0\n",
    "      }\n",
    "      var msgSanitized = msg.replace(\"positif$ \", \"\")\n",
    "      msgSanitized = msgSanitized.replace(\"negatif$ \",\"\")\n",
    "      //Return a tuple\n",
    "      (isHappy, msgSanitized.split(\" \").toSeq)\n",
    "    }\n",
    "  }\n",
    ")\n",
    "var echantillon_ok = echantillon_reformat.filter((_.isSuccess)).map(_.get)\n",
    "//Transformation du texte en nombre par application de la fonction de hachage et mise au format LIBSVM\n",
    "val echantillon_transfo = echantillon_ok.map(\n",
    "  t => (t._1, hashingTF.transform(t._2)))\n",
    "  .map(x => new LabeledPoint((x._1).toDouble, x._2))\n",
    "\n",
    "//ECRITURE EN FICHIER DE SORTIE AU FORMAT LIBSVM\n",
    "MLUtils.saveAsLibSVMFile(echantillon_transfo,\"Data/Vecto_HTF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Vectorisation Word2Vec du fichier non lemmatisé après suppression des StopWords à partir du modèle Word2Vec calculé sur Wikipédia+commentaires français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]                             (1 + 3) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import breeze.linalg.{DenseVector=>BDV, SparseVector=>BSV, Vector=>BV}\n",
       "import org.apache.spark.mllib.linalg.{Vector=>SparkVector}\n",
       "toBreeze: (v: org.apache.spark.mllib.linalg.Vector)breeze.linalg.Vector[Double]\n",
       "fromBreeze: (bv: breeze.linalg.Vector[Double])org.apache.spark.mllib.linalg.Vector\n",
       "add: (v1: org.apache.spark.mllib.linalg.Ve...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// VECTORISATION AVEC WORD2VEC\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "def toBreeze(v:SparkVector) = BV(v.toArray)\n",
    "def fromBreeze(bv:BV[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:SparkVector, v2:SparkVector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:SparkVector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES\n",
    "val echantillon_filtreDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_brut_SWR\")\n",
    "\n",
    "//CHARGEMENT DU MODELE WORD2VEC\n",
    "//val word2vec = new Word2Vec()\n",
    "val w2vModel = Word2VecModel.load(sc, \"modele/Word2VecFR_complet\")\n",
    "\n",
    "//MISE EN FORME DU MODELE LU\n",
    "val vectors = w2vModel.getVectors.mapValues(vv => Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "\n",
    "// transmettre la map aux noeuds de calcul\n",
    "val bVectors = sc.broadcast(vectors)\n",
    "\n",
    "//TAILLE DES VECTEURS WORD2VEC\n",
    "val vectSize = 100\n",
    "\n",
    "//CONSTRUCTION DES VECTEURS ASSOCIES AUX COMMENTAIRES DE L'ECHANTILLON\n",
    "val comment2vec_comptage = echantillon_filtreDF.rdd.map{ row => (row.getAs[String](0)) }.filter(sentence => sentence.length >= 1)\n",
    "    .map(sentence => sentence.toLowerCase.split(\" \"))\n",
    "    .map(wordSeq => { \n",
    "                     Try {                    \n",
    "                          var isHappy:Int = -1\n",
    "                          var vSum = Vectors.zeros(vectSize)\n",
    "                          var vNb = 0\n",
    "                          wordSeq.foreach {word =>\n",
    "                                           if(word.length >= 2) {\n",
    "                                                                 bVectors.value.get(word).foreach {v =>\n",
    "                                                                                                   vSum = add(v, vSum)\n",
    "                                                                                                   vNb += 1\n",
    "                                                                                                  }\n",
    "                                                                }\n",
    "                                           if(word.contains(\"positif$\")){isHappy = 1}\n",
    "                                           else if(word.contains(\"negatif$\")){isHappy = 0}\n",
    "                                          }\n",
    "                          if (vNb != 0) {\n",
    "                                         vSum = scalarMultiply(1.0 / vNb, vSum)\n",
    "                                        }\n",
    "                          (isHappy, vSum, vSum.numNonzeros)\n",
    "                         }\n",
    "                       }\n",
    "                     )\n",
    "\n",
    "//FILTRAGE DES VECTEURS NULS\n",
    "//Création du dataframe associé aux vecteurs Word2vec\n",
    "val echantillon_comptage = comment2vec_comptage.filter(_.isSuccess).map(_.get).toDF(\"Label\", \"Vecteur\", \"Nb\")\n",
    "//Création de la table associée aux vecteurs Word2vec\n",
    "echantillon_comptage.createOrReplaceTempView(\"comptage_vecteur\")\n",
    "//Récupération des vecteurs non nuls\n",
    "val echantillon_filtre = spark.sql(\"select Label, Vecteur from comptage_vecteur where Nb > 0\")\n",
    "//Mise au format RDD\n",
    "val echantillon_filtreRDD = echantillon_filtre.rdd.map(x => (x.getAs[Int](0), x.getAs[Vector](1)))\n",
    "\n",
    "//CONVERSION AU FORMAT LIBSVM\n",
    "val echantillon_transfo = echantillon_filtreRDD.map(x => new LabeledPoint((x._1).toDouble, x._2))\n",
    "\n",
    "//ECRITURE EN FICHIER DE SORTIE AU FORMAT LIBSVM\n",
    "MLUtils.saveAsLibSVMFile(echantillon_transfo,\"Data/Vecto_Word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Vectorisation Word2Vec du fichier non lemmatisé après suppression des StopWords à partir du modèle Word2Vec calculé uniquement sur nos commentaires français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import breeze.linalg.{DenseVector=>BDV, SparseVector=>BSV, Vector=>BV}\n",
       "import org.apache.spark.mllib.linalg.{Vector=>SparkVector}\n",
       "toBreeze: (v: org.apache.spark.mllib.linalg.Vector)breeze.linalg.Vector[Double]\n",
       "fromBreeze: (bv: breeze.linalg.Vector[Double])org.apache.spark.mllib.linalg.Vector\n",
       "add: (v1: org.apache.spark.mllib.linalg.Ve...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// VECTORISATION AVEC WORD2VEC\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "def toBreeze(v:SparkVector) = BV(v.toArray)\n",
    "def fromBreeze(bv:BV[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:SparkVector, v2:SparkVector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:SparkVector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES\n",
    "val echantillon_filtreDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_brut_SWR\")\n",
    "\n",
    "//CHARGEMENT DU MODELE WORD2VEC\n",
    "//val word2vec = new Word2Vec()\n",
    "val w2vModel = Word2VecModel.load(sc, \"modele/Word2VecFR_comment\")\n",
    "\n",
    "//MISE EN FORME DU MODELE LU\n",
    "val vectors = w2vModel.getVectors.mapValues(vv => Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "\n",
    "// transmettre la map aux noeuds de calcul\n",
    "val bVectors = sc.broadcast(vectors)\n",
    "\n",
    "//TAILLE DES VECTEURS WORD2VEC\n",
    "val vectSize = 100\n",
    "\n",
    "//CONSTRUCTION DES VECTEURS ASSOCIES AUX COMMENTAIRES DE L'ECHANTILLON\n",
    "val comment2vec_comptage = echantillon_filtreDF.rdd.map{ row => (row.getAs[String](0)) }.filter(sentence => sentence.length >= 1)\n",
    "    .map(sentence => sentence.toLowerCase.split(\" \"))\n",
    "    .map(wordSeq => { \n",
    "                     Try {                    \n",
    "                          var isHappy:Int = -1\n",
    "                          var vSum = Vectors.zeros(vectSize)\n",
    "                          var vNb = 0\n",
    "                          wordSeq.foreach {word =>\n",
    "                                           if(word.length >= 2) {\n",
    "                                                                 bVectors.value.get(word).foreach {v =>\n",
    "                                                                                                   vSum = add(v, vSum)\n",
    "                                                                                                   vNb += 1\n",
    "                                                                                                  }\n",
    "                                                                }\n",
    "                                           if(word.contains(\"positif$\")){isHappy = 1}\n",
    "                                           else if(word.contains(\"negatif$\")){isHappy = 0}\n",
    "                                          }\n",
    "                          if (vNb != 0) {\n",
    "                                         vSum = scalarMultiply(1.0 / vNb, vSum)\n",
    "                                        }\n",
    "                          (isHappy, vSum, vSum.numNonzeros)\n",
    "                         }\n",
    "                       }\n",
    "                     )\n",
    "\n",
    "//FILTRAGE DES VECTEURS NULS\n",
    "//Création du dataframe associé aux vecteurs Word2vec\n",
    "val echantillon_comptage = comment2vec_comptage.filter(_.isSuccess).map(_.get).toDF(\"Label\", \"Vecteur\", \"Nb\")\n",
    "//Création de la table associée aux vecteurs Word2vec\n",
    "echantillon_comptage.createOrReplaceTempView(\"comptage_vecteur\")\n",
    "//Récupération des vecteurs non nuls\n",
    "val echantillon_filtre = spark.sql(\"select Label, Vecteur from comptage_vecteur where Nb > 0\")\n",
    "//Mise au format RDD\n",
    "val echantillon_filtreRDD = echantillon_filtre.rdd.map(x => (x.getAs[Int](0), x.getAs[Vector](1)))\n",
    "\n",
    "//CONVERSION AU FORMAT LIBSVM\n",
    "val echantillon_transfo = echantillon_filtreRDD.map(x => new LabeledPoint((x._1).toDouble, x._2))\n",
    "\n",
    "//ECRITURE EN FICHIER DE SORTIE AU FORMAT LIBSVM\n",
    "MLUtils.saveAsLibSVMFile(echantillon_transfo,\"Data/Vecto_Word2vecC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ## Vectorisation CountVectorizer du fichier  lemmatisé après suppression des StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_filtreDF = [Liste_mots: string]\n",
       "sqlContext = org.apache.spark.sql.SQLContext@4a7dd347\n",
       "echantillon_filtreDF_Array = [Liste_mots: array<string>]\n",
       "echantillonRDD = MapPartitionsRDD[52] at rdd at <console>:72\n",
       "echantillon_reformat = MapPartitionsRDD[53] at map at <console...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n",
       "import org.apache.spark.mllib.linalg.{Vector=>SparkVector}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[53] at map at <console>:74"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES FILTRE AU FORMAT STRING\n",
    "val echantillon_filtreDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Data/echantillon_lemmatise_SWR\")\n",
    "\n",
    "//LECTURE DU FICHIER D'ENTREE DE L'ECHANTILLON DE COMMENTAIRES EVALUES FILTRE AU FORMAT ARRAY[STRING]\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val echantillon_filtreDF_Array = sqlContext\n",
    "  .read\n",
    "  .format(\"com.databricks.spark.csv\")\n",
    "  .option(\"header\", true)\n",
    "  .csv(\"Data/echantillon_filtre_SWR_V2\")\n",
    "  .withColumn(\"Liste_mots\", split($\"Liste_mots\", \" \"))\n",
    "\n",
    "\n",
    "val echantillonRDD = echantillon_filtreDF.rdd\n",
    "//Constitution du message avec le label\n",
    "val echantillon_reformat = echantillonRDD.map(\n",
    "  row =>{\n",
    "    Try{\n",
    "      val msg = row.toString.toLowerCase()\n",
    "      var isHappy:Int = 0\n",
    "      if(msg.contains(\"positif$\")){\n",
    "        isHappy = 1\n",
    "      }else if(msg.contains(\"negatif$\")){\n",
    "        isHappy = 0\n",
    "      }\n",
    "      var msgSanitized = msg.replace(\"positif$ \", \"\")\n",
    "      msgSanitized = msgSanitized.replace(\"negatif$ \",\"\")\n",
    "      //Return a tuple\n",
    "      (isHappy, msgSanitized.split(\" \").toSeq)\n",
    "    }\n",
    "  }\n",
    ")\n",
    "//Filtrage pour ne garder que les utils de type \"is_success\" et conversion au format RDD\n",
    "var echantillon_ok = echantillon_reformat.filter(_.isSuccess).map(_.get)\n",
    "//Mise au format dataframe avec les bons noms de colonne\n",
    "val echantillon_ok_DF = echantillon_ok.toDF(\"Label\", \"Liste_mots\")\n",
    "//VECTORISATION PAR COUNTVECTORIZER\n",
    "//Définition des paramètres pour l'outil de vectorisation\n",
    "//setMinDF: Paramètre Specifiant le nombre minimum de documents different dans lesquels un mot doit apparaître pour être pris en compte.\n",
    "val vectorizer = new CountVectorizer()\n",
    "  .setInputCol(\"Liste_mots\")\n",
    "  .setOutputCol(\"vecteurs\")\n",
    "  .setVocabSize(10000)\n",
    "  .setMinDF(5)\n",
    "  .fit(echantillon_ok_DF)\n",
    "\n",
    "//Sauvegarde du modèle pour réutilisation ultérieure\n",
    "vectorizer.save(\"modele/CountVectorizer\")\n",
    "//Transformation des mots en vecteurs\n",
    "val echantillon_vect = vectorizer.transform(echantillon_ok_DF)\n",
    "echantillon_vect.createOrReplaceTempView(\"echantillon_vect\")\n",
    "//Conversion du format Integer au format Double pour le label\n",
    "val echantillon_vect_conv = spark.sql(\"select cast(Label as Double) as label, vecteurs from echantillon_vect\")\n",
    "//Mise au format LIBSVM\n",
    "val echantillon_transfo = echantillon_vect_conv.select(\"label\",\"vecteurs\").rdd.map(row => LabeledPoint(\n",
    "                                                             row.getAs[Double](\"label\"),\n",
    "                                                             org.apache.spark.mllib.linalg.Vectors.fromML(row.getAs[org.apache.spark.ml.linalg.SparseVector](\"vecteurs\"))\n",
    "                                                                                                      ))\n",
    "\n",
    "//ECRITURE EN FICHIER DE SORTIE AU FORMAT LIBSVM\n",
    "MLUtils.saveAsLibSVMFile(echantillon_transfo,\"Data/Vecto_Countvectorizer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
