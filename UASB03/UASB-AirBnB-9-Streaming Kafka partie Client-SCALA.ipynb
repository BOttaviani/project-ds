{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNAM UASB03 - CERTIFICATION ANALYSE DE DONNEES MASSIVES\n",
    "## Projet d'analyse de sentiment sur les commentaires Airbnb en français\n",
    "\n",
    "***\n",
    "Notebook Scala de réception et traitements des commentaires du site reçu en streaming via Kafka.\n",
    "Le traitement consiste à :\n",
    "* recevoir les commentaires de langue française via SparkStreaming sur un Topic Kafka\n",
    "* effectuer les mêmes transformations textuelles que pour l'analyse du modèle : mise en forme, suppression des StopWords\n",
    "* vectorisation avec 2 techniques optimales trouvées pour le modèle = Word2Vec pour les commentaires de moins de 300 caractères et HashingTF pour les commentaires de plus de 300 caractères\n",
    "* application du modèle optimal Spark identifié lors de la modélisation = SVM pour les 2 vectorisations\n",
    "* enregistrement du commentaire avec l'étiquetage associé dans une base MongoDB pour pouvoir réutiliser les résultats en visualisation et pour la mise en place d'un mécanisme d'amélioration continue\n",
    "* envoi des commentaires identifiés comme étant négatifs à un autre Topic Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ##  Import des librairies, paramétrage et définition de la fonction de lémmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kafka.serializer.StringDecoder\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions.concat_ws\n",
    "import org.apache.spark.ml.feature.HashingTF\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "def toBreeze(v:SparkVector) = BV(v.toArray)\n",
    "def fromBreeze(bv:BV[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:SparkVector, v2:SparkVector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:SparkVector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "import com.mongodb.spark.sql._\n",
    "import org.bson.Document\n",
    "\n",
    "import org.apache.kafka.clients.producer._\n",
    "import java.util.Properties\n",
    "\n",
    "import java.util\n",
    "\n",
    "\n",
    "// Paramétrage de la session Spark\n",
    "val spark = SparkSession.builder()\n",
    "      .master(\"local[3]\")\n",
    "      .appName(\"MongoSparkConnectorIntro\")\n",
    "      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/airbnb.Commentaires_evalues\")\n",
    "      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/airbnb.Commentaires_evalues\")\n",
    "      .getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "@transient val ssc = new StreamingContext(sc, Minutes(1))\n",
    "\n",
    "// Définition du Consumer Kakka ayant les messages en entrée du Streaming\n",
    "val topicsSet = \"AirBnb_income_fr\".split(\",\").toSet\n",
    "val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n",
    "@transient val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n",
    "      ssc, kafkaParams, topicsSet\n",
    "    )\n",
    "// Définition des propriété du Producer Kafka pour enregistré les messages insatisfaits\n",
    "val props = new Properties()\n",
    "    props.put(\"bootstrap.servers\", \"localhost:9092\")\n",
    "    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "// CHARGEMENT DES STOPWORDS\n",
    "val stopwords = sc.textFile(\"Data/French_stop_words\").collect()\n",
    "\n",
    "//CHARGEMENT DES MODELES SPARK MLIB PRE-CALCULE EN PHASE DE MODELISATION\n",
    "val SVMModel_HTF = SVMModel.load(sc, \"modele/SVM_HTF/\")\n",
    "val SVMModel_W2V = SVMModel.load(sc, \"modele/SVM_W2VC2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Réception des données, transformation, prédiction et enregistrement sur MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.foreachRDD { rdd =>  \n",
    "\n",
    "    //définition du format Json en entrée du streaming\n",
    "    val schema = new StructType()\n",
    "      .add(\"comment_lemm\", StringType)\n",
    "      .add(\"comment_length\", LongType)\n",
    "      .add(\"comments\", StringType)\n",
    "      .add(\"date\", LongType)\n",
    "      .add(\"id\", LongType)\n",
    "      .add(\"langue\", StringType)\n",
    "      .add(\"lg_proba\", DoubleType)\n",
    "      .add(\"listing_id\", LongType)\n",
    "      .add(\"reviewer_id\", LongType)\n",
    "      .add(\"reviewer_name\", StringType)\n",
    "\n",
    " //Lecture du rdd avec le schéma Json dans un dataframe\n",
    "val df = sqlContext.read.schema(schema).json(rdd.map(x => x._2).toDS)\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "//séparation des messages de +/- de 300 caractères    \n",
    "val df_under_300 = spark.sql(\"select * from df where comment_length <=300\")\n",
    "val df_over_300 = spark.sql(\"select * from df where comment_length >300\")\n",
    "\n",
    "//Tokenization du message initial pour les moins de 300 caractères    \n",
    "val tokenizer_under_300 = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"comments\")\n",
    "  .setOutputCol(\"mots\")\n",
    "val tokenized_under_300 = tokenizer_under_300.transform(df_under_300)\n",
    "\n",
    "//Tokenization du message lemmatisé pour les plus de 300 caractères    \n",
    "val tokenizer_over_300 = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"comment_lemm\")\n",
    "  .setOutputCol(\"mots\")\n",
    "val tokenized_over_300 = tokenizer_over_300.transform(df_over_300)\n",
    "\n",
    "// Suppression des StopWords dans les 2 cas\n",
    "val remover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords) \n",
    "  .setInputCol(\"mots\")\n",
    "  .setOutputCol(\"mots_filtres\")\n",
    "\n",
    "//Création des dataframe des mots filtrés\n",
    "val filtrage_under_300 = remover.transform(tokenized_under_300)\n",
    "val filtrage_over_300 = remover.transform(tokenized_over_300)\n",
    "\n",
    "//Vectorisation HashingTF pour les plus de 300 caractères\n",
    "val hashingTF = new HashingTF()\n",
    "   .setNumFeatures(12000)\n",
    "   .setInputCol(\"mots_filtres\")\n",
    "   .setOutputCol(\"features\")\n",
    "    \n",
    "val hashingTF_over_300 = hashingTF.transform(filtrage_over_300)\n",
    "val hashingTF_transfo = MLUtils.convertVectorColumnsFromML(hashingTF_over_300, \"features\")  \n",
    "val feature_HTF = hashingTF_transfo.select(\"features\").rdd\n",
    "\n",
    "// Application du modèle SVM pour HashingTF sur les plus de 300 caratères\n",
    "val prediction_SVM_HTF = SVMModel_HTF.predict(feature_HTF.map(row =>row.getAs[org.apache.spark.mllib.linalg.Vector](\"features\")))\n",
    "\n",
    "// Vectorisation Word2Vec pour les moins de 300 caractères\n",
    "//CHARGEMENT DU MODELE WORD2VEC\n",
    "val w2vModel = Word2VecModel.load(sc,\"modele/Word2VecFR_comment\")\n",
    "// Transmission des vecteurs Word2Vec aux noeuds de calcul\n",
    "//MISE EN FORME DU MODELE LU\n",
    "val vectors = w2vModel.getVectors.mapValues(vv => Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "val bVectors = sc.broadcast(vectors)\n",
    "//TAILLE DES VECTEURS WORD2VEC\n",
    "val vectSize = 100\n",
    "// TRANSFORMATION DE LA LISTE DES MOTS FILTRES EN STRING\n",
    "filtrage_under_300.select(\"mots_filtres\").createOrReplaceTempView(\"test\")\n",
    "val text_under_300=sqlContext.sql(\"select concat_ws(\\\" \\\",mots_filtres) as mots_filtres from test\").rdd\n",
    "//CONSTRUCTION DES VECTEURS ASSOCIES AUX COMMENTAIRES DE L'ECHANTILLON\n",
    "val comment2vec_comptage = text_under_300.map{ row => (row.getAs[String](0)) }.filter(sentence => sentence.length >= 1)\n",
    "    .map(sentence => sentence.toLowerCase.split(\" \"))\n",
    "    .map(wordSeq => { \n",
    "                     Try {                    \n",
    "                          var vSum = Vectors.zeros(vectSize)\n",
    "                          var vNb = 0\n",
    "                          wordSeq.foreach {word =>\n",
    "                                           if(word.length >= 2) {\n",
    "                                                                 bVectors.value.get(word).foreach {v =>\n",
    "                                                                                                   vSum = add(v, vSum)\n",
    "                                                                                                   vNb += 1\n",
    "                                                                                                  }\n",
    "                                                                }\n",
    "                                          }\n",
    "                          if (vNb != 0) {\n",
    "                                         vSum = scalarMultiply(1.0 / vNb, vSum)\n",
    "                                        }\n",
    "                          ( vSum, vSum.numNonzeros)\n",
    "                         }\n",
    "                       }\n",
    "                     )\n",
    "//FILTRAGE DES VECTEURS NULS\n",
    "val echantillon_comptage = comment2vec_comptage.filter(_.isSuccess).map(_.get).toDF( \"Vecteur\", \"Nb\")\n",
    "echantillon_comptage.createOrReplaceTempView(\"comptage_vecteur\")\n",
    "//Récupération des vecteurs non nuls\n",
    "val echantillon_filtre = spark.sql(\"select Vecteur from comptage_vecteur where Nb > 0\")\n",
    "\n",
    "val V_W2V = echantillon_filtre.select(\"Vecteur\").rdd\n",
    "\n",
    "// Application du modèle SVM pour Word2Vecc Corpus2 sur les moins de 300 caratères \n",
    "val prediction_SVM_W2V = SVMModel_W2V.predict(V_W2V.map(row =>row.getAs[org.apache.spark.mllib.linalg.Vector](\"Vecteur\")))\n",
    "\n",
    "// jointure de la prédiction avec le dataframe d'origine pour les - de 300\n",
    "val df_under_300_2 = sqlContext.createDataFrame(\n",
    "  df_under_300.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(df_under_300.schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val df_pred_W2V=sqlContext.createDataFrame(\n",
    "  prediction_SVM_W2V.toDF().rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(prediction_SVM_W2V.toDF().schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val result_under_300 = df_under_300_2.join(df_pred_W2V, Seq(\"columnindex\")).drop(\"columnindex\")\n",
    "\n",
    "// jointure de la prédiction avec le dataframe d'origine pour les + de 300\n",
    "val df_over_300_2 = sqlContext.createDataFrame(\n",
    "  df_over_300.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(df_over_300.schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val df_pred_HTF=sqlContext.createDataFrame(\n",
    "  prediction_SVM_HTF.toDF().rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(prediction_SVM_HTF.toDF().schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val result_over_300 = df_over_300_2.join(df_pred_HTF, Seq(\"columnindex\")).drop(\"columnindex\")\n",
    "\n",
    "// Union des résultats des 2 modèles appliqués\n",
    "val result=result_over_300.union(result_under_300)\n",
    "result.show()\n",
    "//Sauvegarde du résultat dans une base MongoDB\n",
    "result.saveToMongoDB()\n",
    "\n",
    "result.createOrReplaceTempView(\"result\")\n",
    "\n",
    "// Sélection des insatisfaits\n",
    "val insatisfaits = spark.sql(\"select * from result where value=0\")\n",
    "\n",
    "//Envoi des insatisfaits dans le Topik Kafka AirBnb_insatisfaits_fr\n",
    "insatisfaits.selectExpr(\"to_json(struct(*)) AS value\").foreach{row=>{\n",
    "  val producer = new KafkaProducer[String, String](props)\n",
    "  val kMessage=new ProducerRecord[String,String](\"AirBnb_insatisfaits_fr\",row.getString(0))\n",
    " producer.send(kMessage)\n",
    " }}\n",
    "\n",
    "    \n",
    "};\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Arrêt du streaming avant la fin du traitement du flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
