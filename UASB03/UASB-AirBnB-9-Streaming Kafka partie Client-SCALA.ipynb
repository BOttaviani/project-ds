{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNAM UASB03 - CERTIFICATION ANALYSE DE DONNEES MASSIVES\n",
    "## Projet d'analyse de sentiment sur les commentaires Airbnb en français\n",
    "\n",
    "***\n",
    "Notebook Scala de réception et traitements des commentaires du site reçu en streaming via Kafka.\n",
    "Le traitement consiste à :\n",
    "* recevoir les commentaires de langue française via SparkStreaming\n",
    "* effectuer les mêmes transformations textuelles que pour l'analyse du modèle : mise en forme, suppression des StopWords\n",
    "* vectorisation avec la technique optimale trouvée pour le modèle = CountVectorizer\n",
    "* application du modèle optimal Spark identifié lors de la modélisation = SVM\n",
    "* enregistrement du commentaire avec l'étiquetage associé dans une base MongoDB pour pouvoir réutiliser les résultats en visualisation et pour la mise en place d'un mécanisme d'amélioration continue\n",
    "\n",
    "# A REVOIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ##  Import des librairies, paramétrage et définition de la fonction de lémmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kafka.serializer.StringDecoder\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions.concat_ws\n",
    "import org.apache.spark.ml.feature.HashingTF\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import scala.util.{Success, Try}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "def toBreeze(v:SparkVector) = BV(v.toArray)\n",
    "def fromBreeze(bv:BV[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:SparkVector, v2:SparkVector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:SparkVector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "import com.mongodb.spark.sql._\n",
    "import org.bson.Document\n",
    "\n",
    "import org.apache.kafka.clients.producer._\n",
    "import java.util.Properties\n",
    "\n",
    "import java.util\n",
    "\n",
    "\n",
    "// Paramétrage de la session Spark\n",
    "val spark = SparkSession.builder()\n",
    "      .master(\"local[3]\")\n",
    "      .appName(\"MongoSparkConnectorIntro\")\n",
    "      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/test.Comments\")\n",
    "      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/test.Comments\")\n",
    "      .getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "@transient val ssc = new StreamingContext(sc, Minutes(2))\n",
    "\n",
    "// Définition du Consumer Kakka ayant les messages en entrée du Streaming\n",
    "val topicsSet = \"AirBnb_income_fr\".split(\",\").toSet\n",
    "val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n",
    "@transient val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n",
    "      ssc, kafkaParams, topicsSet\n",
    "    )\n",
    "// Définition des propriété du Producer Kafka pour enregistré les messages insatisfaits\n",
    "val props = new Properties()\n",
    "    props.put(\"bootstrap.servers\", \"localhost:9092\")\n",
    "    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "// CHARGEMENT DES STOPWORDS\n",
    "val stopwords = sc.textFile(\"Data/French_stop_words\").collect()\n",
    "\n",
    "//CHARGEMENT DES MODELES SPARK MLIB PRE-CALCULE EN PHASE DE MODELISATION\n",
    "val SVMModel_HTF = SVMModel.load(sc, \"modele/SVM_HTF/\")\n",
    "val SVMModel_W2V = SVMModel.load(sc, \"modele/SVM_W2VC2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Réception des données, transformation, prédiction et enregistrement sur MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id|reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "| petit nid fouill...|           652|Un petit nid foui...|1509667200000|208779822|    fr|0.9922103581022618|      3109|    4142888|     Patricia|  1.0|\n",
      "| tout être bien d...|            37|Tout s'est bien d...|1509148800000|207127433|    fr|0.9611431033472262|      3109|   51636494|     Patricia|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id|reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "|            parfaire|             8|            Parfait |1524441600000|256661206|    fr|0.9380402552418183|      5396|   96294256|      Fabrice|  1.0|\n",
      "| être beau et gen...|           119|Frank est beau et...|1502928000000|183406414|    fr|0.9670138132900836|      7397|  143755622|         Ying|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+------------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id|     reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+------------------+-----+\n",
      "| simplement fanta...|           328|Simplement fantas...|1496707200000|158343256|    fr|0.8472969868884476|      9342|   35962847|            Julien|  1.0|\n",
      "| site exceptionne...|           313|Site exceptionnel...|1522368000000|247920899|    fr|0.9481486763226461|      9342|    9389898|           Sylvain|  1.0|\n",
      "| nuit péniche cad...|           384|Première nuit pou...|1521763200000|245638637|    fr|0.9960975645499098|      9342|   88434516|     Jean-Baptiste|  1.0|\n",
      "| regroupement fam...|           389|Regroupement fami...|1514592000000|222641852|    fr|0.9805905387356849|      9342|  155879348|Dominiqueetcorinne|  1.0|\n",
      "| vivre jour pénic...|           472|Vivre quelques jo...|1503100800000|184114775|    fr|0.9960975645499098|      9342|   43693584|          Delphine|  1.0|\n",
      "| péniche être idé...|           309|La péniche est id...|1507420800000|201340644|    fr|0.9883307509178587|      9342|   12575099|        Emmanuelle|  1.0|\n",
      "| logement être pi...|           301|Ce logement est u...|1497744000000|161630894|    fr| 0.994148254456202|      9952|   15603292|              Joel|  1.0|\n",
      "| accueil chaleure...|           314|Un accueil chaleu...|1507075200000|200143554|    fr|0.9960975645499098|      9342|   39712664|             Laure|  1.0|\n",
      "| être dizaine wee...|           328|Nous étions une d...|1513555200000|219611197|    fr|0.9844587446357446|      9342|   99874334|          Benedict|  1.0|\n",
      "| péniche être idé...|           169|La Péniche est id...|1496620800000|157921062|    fr|0.9689896383967046|      9342|   11442000|              Joel|  1.0|\n",
      "| logement atypiqu...|           104|Logement atypique...|1520553600000|241489788|    fr|0.9500790823577394|      9342|  104741884|            Pascal|  1.0|\n",
      "| expérience uniqu...|            90|Expérience unique...|1514332800000|221691665|    fr|0.9844549433885695|      9342|  126159915|            Claude|  1.0|\n",
      "| avoir faire voya...|           105|Nous avons fait u...|1497225600000|160039189|    fr|0.9922103581022618|      9952|  131237029|            Amélie|  1.0|\n",
      "| si chercher loge...|            98|Si vous chercher ...|1511827200000|215189443|    fr|0.9922103581022618|      9342|  130906388|          Fabienne|  1.0|\n",
      "| génial accueil s...|           204|Génial  Accueil s...|1500336000000|171709139|    fr|0.9690579015082744|      9342|  112641625|          Isabelle|  1.0|\n",
      "| super séjour pro...|           152|Super séjour (pro...|1510790400000|212143547|    fr|0.9729259471204419|      9342|   24407872|          Emmanuel|  1.0|\n",
      "| emplacement merv...|           151|Un emplacement me...|1525824000000|262511507|    fr|0.9385764744679582|      9342|   74087790|           Olivier|  1.0|\n",
      "| tres beau placer...|            70|Une tres belle pl...|1506384000000|197624658|    fr| 0.980579139664056|      9342|    1212391|           Yannick|  1.0|\n",
      "| séjour beau péni...|           228|Notre séjour dans...|1511049600000|212850398|    fr|0.9960975645499098|      9342|  100714544|            Maxime|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+------------------+-----+\n",
      "\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id|reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "| avoir être très ...|           332|Nous avons été tr...|1518307200000|233946732|    fr|0.9922103581022618|     10010|   36458839|       Damien|  1.0|\n",
      "| véritable nid bo...|           316|Un véritable nid ...|1503187200000|184805759|    fr|0.9844549433885695|     10270|   21414659|         Loïc|  1.0|\n",
      "| beau appartement...|           107|Bel appartement s...|1505865600000|195627652|    fr|  0.98639284741291|     10010|  139786166|         Loïc|  1.0|\n",
      "| appartement très...|           111|Appartement très ...|1518393600000|234326511|    fr|0.9806018922769911|     10010|   72327038|     Stephane|  1.0|\n",
      "| merci pour genti...|            81|Merci à Pascal po...|1515888000000|227231483|    fr|0.9747654283369352|     10010|    4374180|       Lecroq|  1.0|\n",
      "| petit appartemen...|           120|Un petit appartem...|1493164800000|147305376|    fr|  0.98639284741291|     10270|    1774411|       Pierre|  1.0|\n",
      "| réactivité excel...|           269|Réactivité excell...|1514246400000|221323838|    fr|0.9844549433885695|     10010|   55404969|      Laurent|  1.0|\n",
      "| être exactement ...|           288|L’appartement est...|1509148800000|207143411|    fr|0.9710069622219485|     10010|   16148997|       Arthur|  1.0|\n",
      "| très bien nuit m...|            39|Très bien pour un...|1495065600000|152800453|    fr|0.9960975645499098|     10270|    8431155|         Marc|  1.0|\n",
      "| hôte charmant et...|           160|Hôte charmant et ...|1512345600000|216569557|    fr|0.9767412543941115|     10010|   20650933|        Jonas|  1.0|\n",
      "| appartement très...|           136|Appartement très ...|1504310400000|189598181|    fr|0.9883269512879979|     10270|    6651586|       Cinzia|  1.0|\n",
      "| appartement être...|           257|L'appartement est...|1518912000000|235990712|    fr|0.9272697229825106|     10010|   78224835|          Jos|  1.0|\n",
      "| très bon séjour ...|           146|Très bon séjour a...|1503619200000|186651309|    fr|0.9844549433885695|     10270|   44246659|       Maxime|  1.0|\n",
      "| accueil impeccab...|            76|Accueil impeccabl...|1511395200000|213810651|    fr|0.9767337315251255|     10010|   39058289|        David|  1.0|\n",
      "| appartement être...|           221|L'appartement de ...|1513123200000|218513031|    fr|0.9844435452361164|     10010|   67021872|  Anne-Sophie|  1.0|\n",
      "| appartement conf...|            75|Appartement confo...|1507075200000|200162042|    fr|0.9163177377323339|     10270|   65176334|         Fred|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id|reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "| situer allée cal...|           389|Situé dans une al...|1513382400000|219018907|    fr|0.9922103581022618|     10270|   67079760|         Anne|  1.0|\n",
      "| appartement être...|           310|L'appartement est...|1521936000000|246282793|    fr|0.9786601969374694|     11170|   58480211|       Sylvie|  1.0|\n",
      "| logement être id...|           417|Le logement de Lé...|1517097600000|230388035|    fr|  0.98639284741291|     11170|   91320784|      Martine|  1.0|\n",
      "| appartement fonc...|           314|Appartement fonct...|1526083200000|263466700|    fr|0.9922065583423111|     10270|   13225860|      Richard|  1.0|\n",
      "| avoir louer appa...|           418|J'ai loué l'appar...|1517788800000|232534318|    fr|0.9902686546042372|     10270|    9752743|      Nicolas|  1.0|\n",
      "| petit appartemen...|           345|Ce petit appartem...|1519603200000|238596778|    fr|0.9786791443088556|     10270|   30232779|   Jean-Marie|  1.0|\n",
      "| appartement situ...|           464|Appartement situé...|1519084800000|236761306|    fr|0.9844587446357446|     10270|   11760422|      Lucette|  1.0|\n",
      "| avoir être accue...|           463|Pascal a été accu...|1524614400000|257162900|    fr|0.9883307509178587|     10270|    4648217|      Malaika|  1.0|\n",
      "| comme l’attentio...|            72|Comme chez soi av...|1508803200000|206203836|    fr|  0.98639284741291|     10270|   42932026|   Clémentine|  1.0|\n",
      "| apartement confo...|           105|Apartement confor...|1527984000000|272202995|    fr|0.8944748281373618|     10270|   26896486|     Brigitte|  1.0|\n",
      "| appartement corr...|           187|Un appartement co...|1524009600000|254810778|    fr|0.9709994509632328|     10270|    2263586|      Nicolas|  1.0|\n",
      "| ne peur rêver lo...|           135|On ne peur rêver ...|1497744000000|161660554|    fr|  0.98639284741291|     11170|   64205617|   Jean-Louis|  1.0|\n",
      "| accueil parfait ...|           100|Accueil parfait a...|1518739200000|235295527|    fr|0.8728986852866726|     10270|   30368499|      Ioannis|  1.0|\n",
      "| sejour merveille...|           169|Un sejour merveil...|1511136000000|213192767|    fr|0.8760043992167005|     10270|   96997100|Yan Le Vernoy|  1.0|\n",
      "| très beau appart...|           157|Un très bel appar...|1515024000000|224786911|    fr| 0.986396639607095|     10270|   27483527|        David|  1.0|\n",
      "| être hôte dispon...|            95|Josiane est une h...|1497052800000|159275635|    fr|0.9980468749261779|     10710|   45155703|   Marie-Anne|  1.0|\n",
      "| appartement fidè...|           113|Appartement fidèl...|1512259200000|216231082|    fr|0.9728883226236025|     10270|   26112912|          Joy|  1.0|\n",
      "| super séjour pro...|            95|Super séjour prof...|1516320000000|228352349|    fr|0.9748260763008697|     10270|  158145934|       Maëlle|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+-------------+-----+\n",
      "\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+--------------+-----+\n",
      "|        comment_lemm|comment_length|            comments|         date|       id|langue|          lg_proba|listing_id|reviewer_id| reviewer_name|value|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+--------------+-----+\n",
      "| appartement être...|           338|L'appartement de ...|1497139200000|159644571|    fr|0.9748109366527249|     11213|   45230109|         Alice|  1.0|\n",
      "| petit nid fouill...|           652|Un petit nid foui...|1509667200000|208779822|    fr|0.9922103581022618|      3109|    4142888|      Patricia|  1.0|\n",
      "| communication fl...|           138|Communication flu...|1495065600000|152763495|    fr|0.9922103581022618|     11213|   10496722|       Sylvain|  1.0|\n",
      "| être beau et gen...|           119|Frank est beau et...|1502928000000|183406414|    fr|0.9670138132900836|      7397|  143755622|          Ying|  1.0|\n",
      "|            parfaire|             8|            Parfait |1524441600000|256661206|    fr|0.9380402552418183|      5396|   96294256|       Fabrice|  1.0|\n",
      "| tout être bien d...|            37|Tout s'est bien d...|1509148800000|207127433|    fr|0.9611431033472262|      3109|   51636494|      Patricia|  1.0|\n",
      "| avoir être très ...|           227|Mathieu a été trè...|1495324800000|153521463|    fr|0.9844587446357446|     11213|   36411722|     Véronique|  1.0|\n",
      "| logement adapter...|            81|Logement adapté à...|1496880000000|158755009|    fr|0.9825170418975137|     11213|   17415942|        Marion|  1.0|\n",
      "| appartement être...|           188|L'appartement de ...|1496620800000|158059115|    fr|0.9883307509178587|     11213|  128103969|Pierre-Edouard|  1.0|\n",
      "| mais avoir être ...|            94|Mais tu as été tr...|1499299200000|167264108|    fr|0.9482049801316239|     11213|   16148997|        Arthur|  1.0|\n",
      "+--------------------+--------------+--------------------+-------------+---------+------+------------------+----------+-----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.foreachRDD { rdd =>  \n",
    "\n",
    "    //définition du format Json en entrée du streaming\n",
    "    val schema = new StructType()\n",
    "      .add(\"comment_lemm\", StringType)\n",
    "      .add(\"comment_length\", LongType)\n",
    "      .add(\"comments\", StringType)\n",
    "      .add(\"date\", LongType)\n",
    "      .add(\"id\", LongType)\n",
    "      .add(\"langue\", StringType)\n",
    "      .add(\"lg_proba\", DoubleType)\n",
    "      .add(\"listing_id\", LongType)\n",
    "      .add(\"reviewer_id\", LongType)\n",
    "      .add(\"reviewer_name\", StringType)\n",
    "\n",
    " //Lecture du rdd avec le schéma Json dans un dataframe\n",
    "val df = sqlContext.read.schema(schema).json(rdd.map(x => x._2).toDS)\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "//séparation des messages de +/- de 300 caractères    \n",
    "val df_under_300 = spark.sql(\"select * from df where comment_length <=300\")\n",
    "val df_over_300 = spark.sql(\"select * from df where comment_length >300\")\n",
    "\n",
    "//Tokenization du message initial pour les moins de 300 caractères    \n",
    "val tokenizer_under_300 = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"comments\")\n",
    "  .setOutputCol(\"mots\")\n",
    "val tokenized_under_300 = tokenizer_under_300.transform(df_under_300)\n",
    "\n",
    "//Tokenization du message lemmatisé pour les plus de 300 caractères    \n",
    "val tokenizer_over_300 = new RegexTokenizer()\n",
    "  .setPattern(\" \") // Séparateur entre les mots du commentaire\n",
    "  .setMinTokenLength(4) // Filtre tous les mots du commentaire de longueur <= 4\n",
    "  .setInputCol(\"comment_lemm\")\n",
    "  .setOutputCol(\"mots\")\n",
    "val tokenized_over_300 = tokenizer_over_300.transform(df_over_300)\n",
    "\n",
    "// Suppression des StopWords dans les 2 cas\n",
    "val remover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords) \n",
    "  .setInputCol(\"mots\")\n",
    "  .setOutputCol(\"mots_filtres\")\n",
    "\n",
    "//Création des dataframe des mots filtrés\n",
    "val filtrage_under_300 = remover.transform(tokenized_under_300)\n",
    "val filtrage_over_300 = remover.transform(tokenized_over_300)\n",
    "\n",
    "//Vectorisation HashingTF pour les plus de 300 caractères\n",
    "val hashingTF = new HashingTF()\n",
    "   .setNumFeatures(12000)\n",
    "   .setInputCol(\"mots_filtres\")\n",
    "   .setOutputCol(\"features\")\n",
    "    \n",
    "val hashingTF_over_300 = hashingTF.transform(filtrage_over_300)\n",
    "val hashingTF_transfo = MLUtils.convertVectorColumnsFromML(hashingTF_over_300, \"features\")  \n",
    "val feature_HTF = hashingTF_transfo.select(\"features\").rdd\n",
    "\n",
    "// Application du modèle SVM pour HashingTF sur les plus de 300 caratères\n",
    "val prediction_SVM_HTF = SVMModel_HTF.predict(feature_HTF.map(row =>row.getAs[org.apache.spark.mllib.linalg.Vector](\"features\")))\n",
    "\n",
    "// Vectorisation Word2Vec pour les moins de 300 caractères\n",
    "//CHARGEMENT DU MODELE WORD2VEC\n",
    "val w2vModel = Word2VecModel.load(sc,\"modele/Word2VecFR_comment\")\n",
    "// Transmission des vecteurs Word2Vec aux noeuds de calcul\n",
    "//MISE EN FORME DU MODELE LU\n",
    "val vectors = w2vModel.getVectors.mapValues(vv => Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "val bVectors = sc.broadcast(vectors)\n",
    "//TAILLE DES VECTEURS WORD2VEC\n",
    "val vectSize = 100\n",
    "// TRANSFORMATION DE LA LISTE DES MOTS FILTRES EN STRING\n",
    "filtrage_under_300.select(\"mots_filtres\").createOrReplaceTempView(\"test\")\n",
    "val text_under_300=sqlContext.sql(\"select concat_ws(\\\" \\\",mots_filtres) as mots_filtres from test\").rdd\n",
    "//CONSTRUCTION DES VECTEURS ASSOCIES AUX COMMENTAIRES DE L'ECHANTILLON\n",
    "val comment2vec_comptage = text_under_300.map{ row => (row.getAs[String](0)) }.filter(sentence => sentence.length >= 1)\n",
    "    .map(sentence => sentence.toLowerCase.split(\" \"))\n",
    "    .map(wordSeq => { \n",
    "                     Try {                    \n",
    "                          var vSum = Vectors.zeros(vectSize)\n",
    "                          var vNb = 0\n",
    "                          wordSeq.foreach {word =>\n",
    "                                           if(word.length >= 2) {\n",
    "                                                                 bVectors.value.get(word).foreach {v =>\n",
    "                                                                                                   vSum = add(v, vSum)\n",
    "                                                                                                   vNb += 1\n",
    "                                                                                                  }\n",
    "                                                                }\n",
    "                                          }\n",
    "                          if (vNb != 0) {\n",
    "                                         vSum = scalarMultiply(1.0 / vNb, vSum)\n",
    "                                        }\n",
    "                          ( vSum, vSum.numNonzeros)\n",
    "                         }\n",
    "                       }\n",
    "                     )\n",
    "//FILTRAGE DES VECTEURS NULS\n",
    "val echantillon_comptage = comment2vec_comptage.filter(_.isSuccess).map(_.get).toDF( \"Vecteur\", \"Nb\")\n",
    "echantillon_comptage.createOrReplaceTempView(\"comptage_vecteur\")\n",
    "//Récupération des vecteurs non nuls\n",
    "val echantillon_filtre = spark.sql(\"select Vecteur from comptage_vecteur where Nb > 0\")\n",
    "\n",
    "val V_W2V = echantillon_filtre.select(\"Vecteur\").rdd\n",
    "\n",
    "// Application du modèle SVM pour Word2Vecc Corpus2 sur les moins de 300 caratères \n",
    "val prediction_SVM_W2V = SVMModel_W2V.predict(V_W2V.map(row =>row.getAs[org.apache.spark.mllib.linalg.Vector](\"Vecteur\")))\n",
    "\n",
    "// jointure de la prédiction avec le dataframe d'origine pour les - de 300\n",
    "val df_under_300_2 = sqlContext.createDataFrame(\n",
    "  df_under_300.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(df_under_300.schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val df_pred_W2V=sqlContext.createDataFrame(\n",
    "  prediction_SVM_W2V.toDF().rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(prediction_SVM_W2V.toDF().schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val result_under_300 = df_under_300_2.join(df_pred_W2V, Seq(\"columnindex\")).drop(\"columnindex\")\n",
    "\n",
    "// jointure de la prédiction avec le dataframe d'origine pour les + de 300\n",
    "val df_over_300_2 = sqlContext.createDataFrame(\n",
    "  df_over_300.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(df_over_300.schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val df_pred_HTF=sqlContext.createDataFrame(\n",
    "  prediction_SVM_HTF.toDF().rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n",
    "  StructType(prediction_SVM_HTF.toDF().schema.fields :+ StructField(\"columnindex\", LongType, false))\n",
    ")\n",
    "val result_over_300 = df_over_300_2.join(df_pred_HTF, Seq(\"columnindex\")).drop(\"columnindex\")\n",
    "\n",
    "// Union des résultats des 2 modèles appliqués\n",
    "val result=result_over_300.union(result_under_300)\n",
    "result.show()\n",
    "//Sauvegarde du résultat dans une base MongoDB\n",
    "result.saveToMongoDB()\n",
    "\n",
    "result.createOrReplaceTempView(\"result\")\n",
    "\n",
    "// Sélection des insatisfaits\n",
    "val insatisfaits = spark.sql(\"select * from result where value=0\")\n",
    "\n",
    "//Envoi des insatisfaits dans le Topik Kafka AirBnb_insatisfaits_fr\n",
    "insatisfaits.selectExpr(\"to_json(struct(*)) AS value\").foreach{row=>{\n",
    "  val producer = new KafkaProducer[String, String](props)\n",
    "  val kMessage=new ProducerRecord[String,String](\"AirBnb_insatisfaits_fr\",row.getString(0))\n",
    " producer.send(kMessage)\n",
    " }}\n",
    "\n",
    "    \n",
    "};\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Arrêt du streaming avant la fin du traitement du flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Test des données présentes en base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'Prediction': 1,\n",
      " '_id': ObjectId('5b797c20b726e840e3ed2c3b'),\n",
      " 'comment_lemm': ' tout être bien dérouler merci bien',\n",
      " 'comments': \"Tout s'est bien déroulé Merci bien PG\",\n",
      " 'date': datetime.datetime(2017, 10, 28, 0, 0),\n",
      " 'id': 207127433,\n",
      " 'langue': 'fr',\n",
      " 'lg_proba': 0.9611431033472262,\n",
      " 'listing_id': 3109,\n",
      " 'reviewer_id': 51636494,\n",
      " 'reviewer_name': 'Patricia'}\n"
     ]
    }
   ],
   "source": [
    "# connection à la base MongoDB\n",
    "client = MongoClient('localhost:27017')\n",
    "db=client.test.Comments \n",
    "\n",
    "\n",
    "# récupération des données de la collection\n",
    "data = db.find({\"Prediction\":1})\n",
    "nb = data.count()\n",
    "# display the data\n",
    "print(nb)\n",
    "for i in data:\n",
    "    p.pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adapté des scripts du projet GitHub suivant : Stream-Processing-using-PySpark-and-MongoDB--master\n",
    "\n",
    "https://github.com/vipulkrishnanmd/Stream-Processing-using-PySpark-and-MongoDB-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
