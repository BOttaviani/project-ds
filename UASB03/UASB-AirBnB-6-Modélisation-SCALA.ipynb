{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNAM UASB03 - CERTIFICATION ANALYSE DE DONNEES MASSIVES\n",
    "## Projet d'analyse de sentiment sur les commentaires Airbnb en français\n",
    "\n",
    "***\n",
    "Notebook Scala de modélisation à partir des données vectorisées précedemment constituées.\n",
    "\n",
    "4 modèles sont testées sur les 4 vectorisations précédentes :\n",
    "- GradientBoostedTrees\n",
    "- SVMWithSGD\n",
    "- LogisticRegressionWithLBFGS\n",
    "- NaiveBayes\n",
    "\n",
    "sur les 4 vectorisations précédentes :\n",
    "- HashingTF\n",
    "- Word2Vec Corpus 1\n",
    "- Word2Vec Corpus 2\n",
    "- CountVectorizer\n",
    "\n",
    "NB : sauf NaiveBayes qui n'est pas testé sur les Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>Gradient Boosting sur la vectorisation Hashing TF </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "//CONTEXTE DE TRAVAIL\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 282:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[6] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[7] at randomSplit at <console>:37, MapPartitionsRDD[8] at randomSplit at <console>:37)\n",
       "trainingData = MapPartitionsRDD[7] at randomSplit at <console>:37\n",
       "validationData = MapPartitionsRDD[8] at randomSplit at <console>:37\n",
       "boostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@1aafffc7,org.apache.spark....\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@1aafffc7,org.apache.spark.mllib.tree.loss.LogLoss$@38c697b2,20,0.1,0.001)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_HTF\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION DU MODELE\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.setNumIterations(20) //number of passes over our training data\n",
    "boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n",
    "boostingStrategy.treeStrategy.setMaxDepth(5)\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val modele = GradientBoostedTrees.train(trainingData, boostingStrategy)\n",
    "// Sauvegarde du modèle\n",
    "modele.save(sc, \"modele/GBT_HTF\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "//pour le jeu de validation\n",
    "var labelAndPredsValid = validationData.map { point =>\n",
    "  val prediction = modele.predict(point.features)\n",
    "  Tuple2(point.label, prediction)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_GBT_HTF = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@62b8daba\n",
       "auPRC_GBT_HTF = 98.85000000000001\n",
       "auROC_GBT_HTF = 91.45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "91.45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_GBT_HTF = new BinaryClassificationMetrics(labelAndPredsValid)\n",
    "val auPRC_GBT_HTF = 100 * (metrics_GBT_HTF.areaUnderPR() - metrics_GBT_HTF.areaUnderPR() % 0.0001) \n",
    "val auROC_GBT_HTF = 100 * (metrics_GBT_HTF.areaUnderROC() - metrics_GBT_HTF.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>Gradient Boosting sur la vectorisation Word2Vec Corpus 1 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 582:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[594] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[595] at randomSplit at <console>:44, MapPartitionsRDD[596] at randomSplit at <console>:44)\n",
       "trainingData = MapPartitionsRDD[595] at randomSplit at <console>:44\n",
       "validationData = MapPartitionsRDD[596] at randomSplit at <console>:44\n",
       "boostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@5ae26811,org.apa...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@5ae26811,org.apache.spark.mllib.tree.loss.LogLoss$@38c697b2,20,0.1,0.001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vec\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION DU MODELE\n",
    "\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.setNumIterations(20) //number of passes over our training data\n",
    "boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n",
    "boostingStrategy.treeStrategy.setMaxDepth(5)\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val modele = GradientBoostedTrees.train(trainingData, boostingStrategy)\n",
    "// Sauvegarde du modèle\n",
    "modele.save(sc, \"modele/GBT_W2V\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "\n",
    "//pour le jeu de validation\n",
    "var labelAndPredsValid = validationData.map { point =>\n",
    "  val prediction = modele.predict(point.features)\n",
    "  Tuple2(point.label, prediction)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_GBT_W2V = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@2cb445f9\n",
       "auPRC_GBT_W2V = 97.72000000000001\n",
       "auROC_GBT_W2V = 85.31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "85.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_GBT_W2V = new BinaryClassificationMetrics(labelAndPredsValid)\n",
    "val auPRC_GBT_W2V = 100 * (metrics_GBT_W2V.areaUnderPR() - metrics_GBT_W2V.areaUnderPR() % 0.0001) \n",
    "val auROC_GBT_W2V = 100 * (metrics_GBT_W2V.areaUnderROC() - metrics_GBT_W2V.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>Gradient Boosting sur la vectorisation Word2Vec Corpus 2 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 894:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[1182] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[1183] at randomSplit at <console>:44, MapPartitionsRDD[1184] at randomSplit at <console>:44)\n",
       "trainingData = MapPartitionsRDD[1183] at randomSplit at <console>:44\n",
       "validationData = MapPartitionsRDD[1184] at randomSplit at <console>:44\n",
       "boostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@c9d584c,org...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@c9d584c,org.apache.spark.mllib.tree.loss.LogLoss$@38c697b2,20,0.1,0.001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vecC2\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION DU MODELE\n",
    "\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.setNumIterations(20) //number of passes over our training data\n",
    "boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n",
    "boostingStrategy.treeStrategy.setMaxDepth(5)\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val modele = GradientBoostedTrees.train(trainingData, boostingStrategy)\n",
    "// Sauvegarde du modèle\n",
    "modele.save(sc, \"modele/GBT_W2VC2\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "\n",
    "//pour le jeu de validation\n",
    "var labelAndPredsValid = validationData.map { point =>\n",
    "  val prediction = modele.predict(point.features)\n",
    "  Tuple2(point.label, prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_GBT_W2V_C2 = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@339dcbd9\n",
       "auPRC_GBT_W2V_C2 = 98.12\n",
       "auROC_GBT_W2V_C2 = 89.42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "89.42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_GBT_W2V_C2 = new BinaryClassificationMetrics(labelAndPredsValid)\n",
    "val auPRC_GBT_W2V_C2 = 100 * (metrics_GBT_W2V_C2.areaUnderPR() - metrics_GBT_W2V_C2.areaUnderPR() % 0.0001) \n",
    "val auROC_GBT_W2V_C2 = 100 * (metrics_GBT_W2V_C2.areaUnderROC() - metrics_GBT_W2V_C2.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>Gradient Boosting sur la vectorisation CountVectorizer </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1207:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[1770] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[1771] at randomSplit at <console>:44, MapPartitionsRDD[1772] at randomSplit at <console>:44)\n",
       "trainingData = MapPartitionsRDD[1771] at randomSplit at <console>:44\n",
       "validationData = MapPartitionsRDD[1772] at randomSplit at <console>:44\n",
       "boostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@30ffe51f,or...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@30ffe51f,org.apache.spark.mllib.tree.loss.LogLoss$@38c697b2,20,0.1,0.001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Countvectorizer\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION DU MODELE\n",
    "\n",
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.setNumIterations(20) //number of passes over our training data\n",
    "boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n",
    "boostingStrategy.treeStrategy.setMaxDepth(5)\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val modele = GradientBoostedTrees.train(trainingData, boostingStrategy)\n",
    "// Sauvegarde du modèle\n",
    "modele.save(sc, \"modele/GBT_CV\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "//pour le jeu de validation\n",
    "var labelAndPredsValid = validationData.map { point =>\n",
    "  val prediction = modele.predict(point.features)\n",
    "  Tuple2(point.label, prediction)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_GBT_CV = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@ea06793\n",
       "auPRC_GBT_CV = 98.91000000000001\n",
       "auROC_GBT_CV = 92.08000000000001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "92.08000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_GBT_CV = new BinaryClassificationMetrics(labelAndPredsValid)\n",
    "val auPRC_GBT_CV = 100 * (metrics_GBT_CV.areaUnderPR() - metrics_GBT_CV.areaUnderPR() % 0.0001) \n",
    "val auROC_GBT_CV = 100 * (metrics_GBT_CV.areaUnderROC() - metrics_GBT_CV.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>SVM sur la vectorisation Hashing TF </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[2358] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[2359] at randomSplit at <console>:47, MapPartitionsRDD[2360] at randomSplit at <console>:47)\n",
       "trainingData = MapPartitionsRDD[2359] at randomSplit at <console>:47\n",
       "validationData = MapPartitionsRDD[2360] at randomSplit at <console>:47\n",
       "numIterations = 100\n",
       "model = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 12000, numClasses = 2, threshold = None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_HTF\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(trainingData, numIterations)\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/SVM_HTF\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "// Clear the default threshold.\n",
    "model.clearThreshold()\n",
    "// Prédiction sur le jeu de validation\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_SVM_HTF = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@57645e77\n",
       "auPRC_SVM_HTF = 99.36\n",
       "auROC_SVM_HTF = 97.18\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "97.18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_SVM_HTF = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_SVM_HTF = 100 * (metrics_SVM_HTF.areaUnderPR() - metrics_SVM_HTF.areaUnderPR() % 0.0001) \n",
    "val auROC_SVM_HTF = 100 * (metrics_SVM_HTF.areaUnderROC() - metrics_SVM_HTF.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>SVM sur la vectorisation Word2Vec Corpus 1 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1384:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[2609] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[2610] at randomSplit at <console>:49, MapPartitionsRDD[2611] at randomSplit at <console>:49)\n",
       "trainingData = MapPartitionsRDD[2610] at randomSplit at <console>:49\n",
       "validationData = MapPartitionsRDD[2611] at randomSplit at <console>:49\n",
       "numIterations = 100\n",
       "model = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vec\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(trainingData, numIterations)\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/SVM_W2V\")\n",
    "model.clearThreshold()\n",
    "// Prédiction sur le jeu de validation\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_SVM_W2V = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@75c815e1\n",
       "auPRC_SVM_W2V = 95.57\n",
       "auROC_SVM_W2V = 82.77\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "82.77"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_SVM_W2V = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_SVM_W2V = 100 * (metrics_SVM_W2V.areaUnderPR() - metrics_SVM_W2V.areaUnderPR() % 0.0001) \n",
    "val auROC_SVM_W2V = 100 * (metrics_SVM_W2V.areaUnderROC() - metrics_SVM_W2V.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>SVM sur la vectorisation Word2Vec Corpus 2 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[2860] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[2861] at randomSplit at <console>:49, MapPartitionsRDD[2862] at randomSplit at <console>:49)\n",
       "trainingData = MapPartitionsRDD[2861] at randomSplit at <console>:49\n",
       "validationData = MapPartitionsRDD[2862] at randomSplit at <console>:49\n",
       "numIterations = 100\n",
       "model = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vecC2\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(trainingData, numIterations)\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/SVM_W2VC2\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "// Clear the default threshold.\n",
    "model.clearThreshold()\n",
    "// Prédiction sur le jeu de validation\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_SVM_W2V_C2 = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@12ffbddc\n",
       "auPRC_SVM_W2V_C2 = 99.14\n",
       "auROC_SVM_W2V_C2 = 96.28000000000002\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "96.28000000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_SVM_W2V_C2 = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_SVM_W2V_C2 = 100 * (metrics_SVM_W2V_C2.areaUnderPR() - metrics_SVM_W2V_C2.areaUnderPR() % 0.0001) \n",
    "val auROC_SVM_W2V_C2 = 100 * (metrics_SVM_W2V_C2.areaUnderROC() - metrics_SVM_W2V_C2.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####  <font color=blue>SVM sur la vectorisation CountVectorizer </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3111] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[3112] at randomSplit at <console>:49, MapPartitionsRDD[3113] at randomSplit at <console>:49)\n",
       "trainingData = MapPartitionsRDD[3112] at randomSplit at <console>:49\n",
       "validationData = MapPartitionsRDD[3113] at randomSplit at <console>:49\n",
       "numIterations = 100\n",
       "model = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 3444, numClasses = 2, threshold = None"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Countvectorizer\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(trainingData, numIterations)\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/SVM_CV\")\n",
    "\n",
    "//EVALUATION DU MODELE\n",
    "// Clear the default threshold.\n",
    "model.clearThreshold()\n",
    "// Prédiction sur le jeu de validation\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_SVM_CV = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@2114ccb1\n",
       "auPRC_SVM_CV = 99.5\n",
       "auROC_SVM_CV = 97.87\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "97.87"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_SVM_CV = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_SVM_CV = 100 * (metrics_SVM_CV.areaUnderPR() - metrics_SVM_CV.areaUnderPR() % 0.0001) \n",
    "val auROC_SVM_CV = 100 * (metrics_SVM_CV.areaUnderROC() - metrics_SVM_CV.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Régression logistique sur la vectorisation HashingTF </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3362] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[3363] at randomSplit at <console>:54, MapPartitionsRDD[3364] at randomSplit at <console>:54)\n",
       "trainingData = MapPartitionsRDD[3363] at randomSplit at <console>:54\n",
       "validationData = MapPartitionsRDD[3364] at randomSplit at <console>:54\n",
       "model = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeat...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 12000, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_HTF\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION ET APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val model = new LogisticRegressionWithLBFGS()\n",
    "  .setNumClasses(2)\n",
    "  .run(trainingData)\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/LGB_HTF\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_LGB_HTF = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@7547b107\n",
       "auPRC_LGB_HTF = 97.46000000000001\n",
       "auROC_LGB_HTF = 87.69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "87.69"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_LGB_HTF = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_LGB_HTF = 100 * (metrics_LGB_HTF.areaUnderPR() - metrics_LGB_HTF.areaUnderPR() % 0.0001) \n",
    "val auROC_LGB_HTF = 100 * (metrics_LGB_HTF.areaUnderROC() - metrics_LGB_HTF.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Régression logistique sur la vectorisation Word2Vec Corpus 1 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1873:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3469] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[3470] at randomSplit at <console>:53, MapPartitionsRDD[3471] at randomSplit at <console>:53)\n",
       "trainingData = MapPartitionsRDD[3470] at randomSplit at <console>:53\n",
       "validationData = MapPartitionsRDD[3471] at randomSplit at <console>:53\n",
       "model = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeat...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vec\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION ET APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val model = new LogisticRegressionWithLBFGS()\n",
    "  .setNumClasses(2)\n",
    "  .run(trainingData)\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/LGB_W2V\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_LGB_W2V = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@54e52df6\n",
       "auPRC_LGB_W2V = 97.51\n",
       "auROC_LGB_W2V = 87.79\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "87.79"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_LGB_W2V = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_LGB_W2V = 100 * (metrics_LGB_W2V.areaUnderPR() - metrics_LGB_W2V.areaUnderPR() % 0.0001) \n",
    "val auROC_LGB_W2V = 100 * (metrics_LGB_W2V.areaUnderROC() - metrics_LGB_W2V.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Régression logistique sur la vectorisation Word2Vec Corpus 2 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3618] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[3619] at randomSplit at <console>:53, MapPartitionsRDD[3620] at randomSplit at <console>:53)\n",
       "trainingData = MapPartitionsRDD[3619] at randomSplit at <console>:53\n",
       "validationData = MapPartitionsRDD[3620] at randomSplit at <console>:53\n",
       "model = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeat...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 100, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Word2vecC2\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION ET APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val model = new LogisticRegressionWithLBFGS()\n",
    "  .setNumClasses(2)\n",
    "  .run(trainingData)\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/LGB_W2VC2\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_LGB_W2V_C2 = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4ff30623\n",
       "auPRC_LGB_W2V_C2 = 98.38\n",
       "auROC_LGB_W2V_C2 = 91.95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "91.95"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_LGB_W2V_C2 = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_LGB_W2V_C2 = 100 * (metrics_LGB_W2V_C2.areaUnderPR() - metrics_LGB_W2V_C2.areaUnderPR() % 0.0001) \n",
    "val auROC_LGB_W2V_C2 = 100 * (metrics_LGB_W2V_C2.areaUnderROC() - metrics_LGB_W2V_C2.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Régression logistique sur la vectorisation CountVectorizer </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3752] at map at MLUtils.scala:84\n",
       "splits = Array(MapPartitionsRDD[3753] at randomSplit at <console>:53, MapPartitionsRDD[3754] at randomSplit at <console>:53)\n",
       "trainingData = MapPartitionsRDD[3753] at randomSplit at <console>:53\n",
       "validationData = MapPartitionsRDD[3754] at randomSplit at <console>:53\n",
       "model = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeat...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 3444, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Countvectorizer\")\n",
    "\n",
    "//PARTAGE DE L'ECHANTILLON EN JEUX D'APPRENTISSAGE ET DE VALIDATION\n",
    "val splits = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "//CONSTRUCTION ET APPLICATION DU MODELE AU JEU D'APPRENTISSAGE\n",
    "val model = new LogisticRegressionWithLBFGS()\n",
    "  .setNumClasses(2)\n",
    "  .run(trainingData)\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/LGB_CV\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_LGB_CV = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@49d5897d\n",
       "auPRC_LGB_CV = 97.81\n",
       "auROC_LGB_CV = 89.13000000000001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "89.13000000000001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_LGB_CV = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_LGB_CV = 100 * (metrics_LGB_CV.areaUnderPR() - metrics_LGB_CV.areaUnderPR() % 0.0001) \n",
    "val auROC_LGB_CV = 100 * (metrics_LGB_CV.areaUnderROC() - metrics_LGB_CV.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Naïve Bayes sur la vectorisation HashingTF </font> \n",
    " http://www.ijettcs.org/Volume6Issue5/IJETTCS-2017-08-31-7.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3897] at map at MLUtils.scala:84\n",
       "trainingData = MapPartitionsRDD[3898] at randomSplit at <console>:58\n",
       "validationData = MapPartitionsRDD[3899] at randomSplit at <console>:58\n",
       "model = org.apache.spark.mllib.classification.NaiveBayesModel@193002d5\n",
       "predictionAndLabels = MapPartitionsRDD[3922] at map at <console>:66\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3922] at map at <console>:66"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM :RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_HTF\")\n",
    "\n",
    "// Split data into training (70%) and test (30%).\n",
    "val Array(trainingData, validationData) = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val model = NaiveBayes.train(trainingData, lambda = 1.0, modelType = \"multinomial\")\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/ANB_HTF\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "//val predictionAndLabel = test.map(p => (modele.predict(p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_ANB_HTF = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27a6d794\n",
       "auPRC_ANB_HTF = 98.55000000000001\n",
       "auROC_ANB_HTF = 92.28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "92.28"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_ANB_HTF = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_ANB_HTF = 100 * (metrics_ANB_HTF.areaUnderPR() - metrics_ANB_HTF.areaUnderPR() % 0.0001) \n",
    "val auROC_ANB_HTF = 100 * (metrics_ANB_HTF.areaUnderROC() - metrics_ANB_HTF.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Naïve Bayes sur la vectorisation CountVectorizer </font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "echantillon_LIBSVM = MapPartitionsRDD[3959] at map at MLUtils.scala:84\n",
       "trainingData = MapPartitionsRDD[3960] at randomSplit at <console>:57\n",
       "validationData = MapPartitionsRDD[3961] at randomSplit at <console>:57\n",
       "model = org.apache.spark.mllib.classification.NaiveBayesModel@71c12416\n",
       "predictionAndLabels = MapPartitionsRDD[3984] at map at <console>:65\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3984] at map at <console>:65"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//LECTURE D'UN FICHIER AU FORMAT LIBSVM\n",
    "val echantillon_LIBSVM :RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"Data/Vecto_Countvectorizer\")\n",
    "\n",
    "// Split data into training (70%) and test (30%).\n",
    "val Array(trainingData, validationData) = echantillon_LIBSVM.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val model = NaiveBayes.train(trainingData, lambda = 1.0, modelType = \"multinomial\")\n",
    "\n",
    "// Sauvegarde du modèle\n",
    "model.save(sc, \"modele/ANB_CV\")\n",
    "\n",
    "//APPLICATION DU MODELE AU JEU DE VALIDATION\n",
    "val predictionAndLabels = validationData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_ANB_CV = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@489b7b6b\n",
       "auPRC_ANB_CV = 98.70000000000002\n",
       "auROC_ANB_CV = 93.15\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "93.15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CALCUL DES INDICATEURS DE PERFORMANCE\n",
    "val metrics_ANB_CV = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val auPRC_ANB_CV = 100 * (metrics_ANB_CV.areaUnderPR() - metrics_ANB_CV.areaUnderPR() % 0.0001) \n",
    "val auROC_ANB_CV = 100 * (metrics_ANB_CV.areaUnderROC() - metrics_ANB_CV.areaUnderROC() % 0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ####  <font color=blue>Synthèse des métriques </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle GRADIENT BOOSTING - Vectorisation HASHINGTF\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.85000000000001 %\n",
      "Area under ROC = 91.45 %\n",
      "Modèle GRADIENT BOOSTING - Vectorisation WORD2VEC sur corpus wikipédia\n",
      "============================================================\n",
      "Area under precision-recall curve = 97.72000000000001 %\n",
      "Area under ROC = 85.31 %\n",
      "Modèle GRADIENT BOOSTING - Vectorisation WORD2VEC sur corpus commentaire\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.12 %\n",
      "Area under ROC = 89.42 %\n",
      "Modèle GRADIENT BOOSTING - Vectorisation COUNTVECTORIZER\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.91000000000001 %\n",
      "Area under ROC = 92.08000000000001 %\n",
      "Modèle SVM - vectorisation HASHINGTF\n",
      "============================================================\n",
      "Area under precision-recall curve = 99.36 %\n",
      "Area under ROC = 97.18 %\n",
      "Modèle SVM - vectorisation WORD2VEC sur corpus wikipédia\n",
      "============================================================\n",
      "Area under precision-recall curve = 95.57 %\n",
      "Area under ROC = 82.77 %\n",
      "Modèle SVM - vectorisation WORD2VEC sur corpus commentaire\n",
      "============================================================\n",
      "Area under precision-recall curve = 99.14 %\n",
      "Area under ROC = 96.28000000000002 %\n",
      "Modèle SVM - vectorisation COUNTVECTORIZER\n",
      "============================================================\n",
      "Area under precision-recall curve = 99.5 %\n",
      "Area under ROC = 97.87 %\n",
      "Modèle REGRESSION LOGISTIQUE - vectorisation HASHINGTF\n",
      "============================================================\n",
      "Area under precision-recall curve = 97.46000000000001 %\n",
      "Area under ROC = 87.69 %\n",
      "Modèle REGRESSION LOGISTIQUE - vectorisation WORD2VEC sur corpus wikipédia\n",
      "============================================================\n",
      "Area under precision-recall curve = 97.51 %\n",
      "Area under ROC = 87.79 %\n",
      "Modèle REGRESSION LOGISTIQUE - vectorisation WORD2VEC sur corpus commentaire\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.38 %\n",
      "Area under ROC = 91.95 %\n",
      "Modèle REGRESSION LOGISTIQUE - vectorisation COUNTVECTORIZER\n",
      "============================================================\n",
      "Area under precision-recall curve = 97.81 %\n",
      "Area under ROC = 89.13000000000001 %\n",
      "Modèle NAIVE BAYES - vectorisation HASHINGTF\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.55000000000001 %\n",
      "Area under ROC = 92.28 %\n",
      "Modèle NAIVE BAYES - vectorisation COUNTVECTORIZER\n",
      "============================================================\n",
      "Area under precision-recall curve = 98.70000000000002 %\n",
      "Area under ROC = 93.15 %\n"
     ]
    }
   ],
   "source": [
    "//Affichage des métriques\n",
    "println(\"Modèle GRADIENT BOOSTING - Vectorisation HASHINGTF\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_GBT_HTF %\")\n",
    "println(s\"Area under ROC = $auROC_GBT_HTF %\")\n",
    "println(\"Modèle GRADIENT BOOSTING - Vectorisation WORD2VEC sur corpus wikipédia\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_GBT_W2V %\")\n",
    "println(s\"Area under ROC = $auROC_GBT_W2V %\")\n",
    "println(\"Modèle GRADIENT BOOSTING - Vectorisation WORD2VEC sur corpus commentaire\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_GBT_W2V_C2 %\")\n",
    "println(s\"Area under ROC = $auROC_GBT_W2V_C2 %\")\n",
    "println(\"Modèle GRADIENT BOOSTING - Vectorisation COUNTVECTORIZER\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_GBT_CV %\")\n",
    "println(s\"Area under ROC = $auROC_GBT_CV %\")\n",
    "println(\"Modèle SVM - vectorisation HASHINGTF\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_SVM_HTF %\")\n",
    "println(s\"Area under ROC = $auROC_SVM_HTF %\")\n",
    "println(\"Modèle SVM - vectorisation WORD2VEC sur corpus wikipédia\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_SVM_W2V %\")\n",
    "println(s\"Area under ROC = $auROC_SVM_W2V %\")\n",
    "println(\"Modèle SVM - vectorisation WORD2VEC sur corpus commentaire\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_SVM_W2V_C2 %\")\n",
    "println(s\"Area under ROC = $auROC_SVM_W2V_C2 %\")\n",
    "println(\"Modèle SVM - vectorisation COUNTVECTORIZER\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_SVM_CV %\")\n",
    "println(s\"Area under ROC = $auROC_SVM_CV %\")\n",
    "println(\"Modèle REGRESSION LOGISTIQUE - vectorisation HASHINGTF\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_LGB_HTF %\")\n",
    "println(s\"Area under ROC = $auROC_LGB_HTF %\")\n",
    "println(\"Modèle REGRESSION LOGISTIQUE - vectorisation WORD2VEC sur corpus wikipédia\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_LGB_W2V %\")\n",
    "println(s\"Area under ROC = $auROC_LGB_W2V %\")\n",
    "println(\"Modèle REGRESSION LOGISTIQUE - vectorisation WORD2VEC sur corpus commentaire\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_LGB_W2V_C2 %\")\n",
    "println(s\"Area under ROC = $auROC_LGB_W2V_C2 %\")\n",
    "println(\"Modèle REGRESSION LOGISTIQUE - vectorisation COUNTVECTORIZER\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_LGB_CV %\")\n",
    "println(s\"Area under ROC = $auROC_LGB_CV %\")\n",
    "println(\"Modèle NAIVE BAYES - vectorisation HASHINGTF\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_ANB_HTF %\")\n",
    "println(s\"Area under ROC = $auROC_ANB_HTF %\")\n",
    "println(\"Modèle NAIVE BAYES - vectorisation COUNTVECTORIZER\")\n",
    "println(\"============================================================\")\n",
    "println(s\"Area under precision-recall curve = $auPRC_ANB_CV %\")\n",
    "println(s\"Area under ROC = $auROC_ANB_CV %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
