{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNAM UASB03 - CERTIFICATION ANALYSE DE DONNEES MASSIVES\n",
    "## Projet d'analyse de sentiment sur les commentaires Airbnb en français\n",
    "\n",
    "***\n",
    "Notebook Python réception et traitements des commentaires du site reçu en streaming simple.\n",
    "Le traitement consiste à :\n",
    "* recevoir chaque commentaire via SparkStreaming\n",
    "* détecté la langue pour ne conserver que les commentaire reconnu comme français avec une probabilité supérieure à 80%\n",
    "* effectuer les mêmes transformations textuelles que pour l'analyse du modèle : mise en forme, lemmatisation et suppression des StopWords\n",
    "* vectorisation avec la technique optimale trouvée pour le modèle = CountVectorizer\n",
    "* application du modèle optimal Spark identifié lors de la modélisation = SVM\n",
    "* enregistrement du commentaire avec l'étiquetage associé dans une base MongoDB pour pouvoir réutiliser les résultats en visualisation et pour la mise en place d'un mécanisme d'amélioration continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ##  Import des librairies, paramétrage et définition de la fonction de lémmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer,CountVectorizerModel\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from  pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "from pymongo import MongoClient\n",
    "import pprint as p\n",
    "import pandas, json, sys\n",
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "\n",
    "from whatthelang import WhatTheLang\n",
    "wtl = WhatTheLang()\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.2.0 pyspark-shell'\n",
    "\n",
    "# Création d'un spark context, d'un sql context et d'un streaming context\n",
    "sc = SparkContext(\"local[2]\",\"Commentaires AirBnB\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)\n",
    "kafkaStream = KafkaUtils.createStream(ssc, '127.0.0.1:2181', 'spark-streaming', {'AirBnb_income':1})\n",
    "#socket_stream = ssc.socketTextStream(\"localhost\",9999)\n",
    "\n",
    "stopwords = sc.textFile(\"Data/French_stop_words\").collect()\n",
    "\n",
    "def comment_to_lemme(comment):\n",
    "    t=treetaggerwrapper.make_tags(tagger.tag_text(comment))\n",
    "    lemme=''\n",
    "    for i in t:\n",
    "        if type(i)==treetaggerwrapper.Tag:\n",
    "            if i.pos[:3] in ('ADJ', 'ADV', 'INT','KON','NOM','VER'): \n",
    "                if i.lemma !='dns-remplacé':\n",
    "                    if len(i.lemma)>1 :\n",
    "                        lemme =lemme+' '+i.lemma.split('|')[0].lower()\n",
    "    return lemme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Réception des données, transformation, prédiction et enregistrement sur MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Récupération sur une fenêtre d'1s\n",
    "flux = kafkaStream.window( 1 )\n",
    "\n",
    "## Chargement des modèles calculés\n",
    "SVMModel = SVMModel.load(sc, \"modele/SVM_CV/\")\n",
    "CountVector = CountVectorizerModel.load(\"modele/CountVectorizer/\")\n",
    "\n",
    "# Traitement sur chaque ligne du commentaire\n",
    "\n",
    "def TraitementCommentaire(x):\n",
    " # Récupération des RDD dans une liste\n",
    "    x = x.map(lambda y: y[1]).collect()\n",
    "\n",
    " # Gestion de la fin du fichier\n",
    "    if (x == 'Fini'):\n",
    "        # arrêt des context\n",
    "        global ssc\n",
    "        global sqlContext\n",
    "        global sc\n",
    "        ssc.stop()\n",
    "        sqlContext.stop()\n",
    "        sc.stop()\n",
    "        print(\"Le traitement du flux est terminé\")\n",
    "        return 1\n",
    "            \n",
    "    # Connection à la base MongoDB (qui tourne sur Docker)\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db=client.test.Comments \n",
    "    \n",
    "\n",
    "    # Gestion des différents item de la liste\n",
    "    for item in x:\n",
    "        #comment_texte = item[0]\n",
    "        #print(item)\n",
    "        \n",
    "        #transformation en dataframe pandas\n",
    "        #comment = pandas.read_json(comment_texte,typ='series').to_frame().transpose()\n",
    "        comment = pandas.read_json(item)\n",
    "        comment['comments']=comment['comments'].replace(\"\\r\\n\", \"\")\n",
    "        comment['comments']=comment['comments'].replace(\"\\n\", \"\")\n",
    "        comment['comments']=comment['comments'].astype(str)\n",
    "        # Détection de la langue et ajout au dataframe\n",
    "        comment['langage']=comment['comments'].apply(wtl.pred_prob)\n",
    "        comment['langue']=comment['langage'].str[0].str[0].str[0]\n",
    "        comment['lg_proba']=comment['langage'].str[0].str[0].str[1]\n",
    "        #print(comment)\n",
    "        comment=comment.drop(['langage'],axis=1)\n",
    "        comment = comment[(comment['lg_proba']>0.8) & (comment['langue']=='fr')]\n",
    "        comment['comments'] = comment['comments'].apply(lambda x: x.replace('.',' ').replace('@',' ').replace('+',' ').\n",
    "            replace(',',' ').replace(';',' ').replace('!',' ').replace('  ',' '))\n",
    "\n",
    "        comment['comment_lemm'] = comment['comments'].apply(comment_to_lemme)\n",
    "        #print(comment)\n",
    "        #print(comment.shape[0])\n",
    "        test_langue = comment.shape[0]\n",
    "        if (test_langue > 0) :\n",
    "            print(\"test OK\")\n",
    "            #transformation du dataframe Pandas en RDD\n",
    "            CommentDF = sqlContext.createDataFrame(comment[['id','comment_lemm']])\n",
    "            CommentDF.registerTempTable(\"CommentDF\")\n",
    "            \n",
    "            tokenizer = Tokenizer(inputCol=\"comment_lemm\", outputCol=\"words\")\n",
    "            wordsData = tokenizer.transform(CommentDF)\n",
    "            remover = StopWordsRemover(stopWords=stopwords, inputCol=\"words\",outputCol=\"Liste_mots\")\n",
    "            wordsData_SW = remover.transform(wordsData)\n",
    "            #hashingTF = HashingTF(inputCol=\"removed\", outputCol=\"features\", numFeatures=12000)\n",
    "            #hashingTF_model = hashingTF.transform(wordsData_SW)\n",
    "            #hashingTF_transfo = MLUtils.convertVectorColumnsFromML(hashingTF_model, \"features\")\n",
    "            CV_transfo = CountVector.transform(wordsData_SW)\n",
    "            CV_transfo  = CV_transfo.select(col('id'),col('comment_lemm'),col('words'),\n",
    "                  col('Liste_mots'),col('vecteurs').alias('features'))\n",
    "            CV_transfo.take(1)\n",
    "            CV_data = MLUtils.convertVectorColumnsFromML(CV_transfo, \"features\")\n",
    "            CV_data.take(1)\n",
    "            #Vecteur_Apredire=hashingTF_transfo.select(\"features\").take(1)\n",
    "            #Vecteur_Apredire.show(n=1)\n",
    "            #P=Vecteur_Apredire[0].features\n",
    "            \n",
    "            #P=SparseVector(20000, {4543: 1.0, 5284: 1.0, 8353: 1.0, 8809: 1.0, 10935: 1.0, 10983: 1.0})\n",
    "\n",
    "            #prediction_GBT = GBTModel.predict(hashingTF_transfo.rdd.map(lambda x: x.features))\n",
    "            ## Chargement du modèle calculé\n",
    "\n",
    "            prediction_SVM = SVMModel.predict(CV_data.rdd.map(lambda x: x.features))\n",
    "            prediction=prediction_SVM.take(1)\n",
    "\n",
    "            comment['Prediction']=prediction[0]\n",
    "        \n",
    "            print(\"Insertion\")\n",
    "            \n",
    "            # insertion dans MongoDB si Français ou Anglais avec une probabilité à plus de 80%\n",
    "            db.insert_many(comment[(comment['lg_proba']>0.8) & (comment['langue']=='fr')].to_dict('records'))\n",
    "    \n",
    "            return 0\n",
    "    # Fermeture du client MongoDB\n",
    "    client.close()\n",
    "\n",
    "    return 0\n",
    "# Traitement de tous les RDD reçus dans le streaming en appliquant la fonction précédente\n",
    "flux.foreachRDD(TraitementCommentaire)\n",
    "\n",
    "\n",
    "# démarrage du flux\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Arrêt du streaming avant la fin du traitement du flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Test des données présentes en base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'Prediction': 1,\n",
      " '_id': ObjectId('5b797c20b726e840e3ed2c3b'),\n",
      " 'comment_lemm': ' tout être bien dérouler merci bien',\n",
      " 'comments': \"Tout s'est bien déroulé Merci bien PG\",\n",
      " 'date': datetime.datetime(2017, 10, 28, 0, 0),\n",
      " 'id': 207127433,\n",
      " 'langue': 'fr',\n",
      " 'lg_proba': 0.9611431033472262,\n",
      " 'listing_id': 3109,\n",
      " 'reviewer_id': 51636494,\n",
      " 'reviewer_name': 'Patricia'}\n"
     ]
    }
   ],
   "source": [
    "# connection à la base MongoDB\n",
    "client = MongoClient('localhost:27017')\n",
    "db=client.test.Comments \n",
    "\n",
    "\n",
    "# récupération des données de la collection\n",
    "data = db.find({\"Prediction\":1})\n",
    "nb = data.count()\n",
    "# display the data\n",
    "print(nb)\n",
    "for i in data:\n",
    "    p.pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adapté des scripts du projet GitHub suivant : Stream-Processing-using-PySpark-and-MongoDB--master\n",
    "\n",
    "https://github.com/vipulkrishnanmd/Stream-Processing-using-PySpark-and-MongoDB-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
