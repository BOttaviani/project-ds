{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamFields = Vector(StructField(val0,DoubleType,true), StructField(val1,DoubleType,true), StructField(val2,DoubleType,true), StructField(val3,DoubleType,true), StructField(val4,DoubleType,true), StructField(val5,DoubleType,true), StructField(val6,DoubleType,true), StructFiel...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "genSpamFields: (from: Int, to: Int)scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(StructField(val0,DoubleType,true), StructField(val1,DoubleType,true), StructField(val2,DoubleType,true), StructField(val3,DoubleType,true), StructField(val4,DoubleType,true), StructField(val5,DoubleType,true), StructField(val6,DoubleType,true), StructField(val7,DoubleType,true), StructField(val8,DoubleType,true), StructField(val9,DoubleType,true), StructField(val10,DoubleType,true), StructField(val11,DoubleType,true), StructField(val12,DoubleType,true), StructField(val13,DoubleType,true), StructField(val14,DoubleType,true), StructField(val15,DoubleType,true), StructField(val16,DoubleType,true), StructField(val17,DoubleType,true), StructField(val18,DoubleType,true), StructField(val19,DoubleType,true), StructField(val20,DoubleType,true), StructField(val21,DoubleType,true), StructField(val22,DoubleType,true), StructField(val23,DoubleType,true), StructField(val24,DoubleType,true), StructField(val25,DoubleType,true), StructField(val26,DoubleType,true), StructField(val27,DoubleType,true), StructField(val28,DoubleType,true), StructField(val29,DoubleType,true), StructField(val30,DoubleType,true), StructField(val31,DoubleType,true), StructField(val32,DoubleType,true), StructField(val33,DoubleType,true), StructField(val34,DoubleType,true), StructField(val35,DoubleType,true), StructField(val36,DoubleType,true), StructField(val37,DoubleType,true), StructField(val38,DoubleType,true), StructField(val39,DoubleType,true), StructField(val40,DoubleType,true), StructField(val41,DoubleType,true), StructField(val42,DoubleType,true), StructField(val43,DoubleType,true), StructField(val44,DoubleType,true), StructField(val45,DoubleType,true), StructField(val46,DoubleType,true), StructField(val47,DoubleType,true), StructField(val48,DoubleType,true), StructField(val49,DoubleType,true), StructField(val50,DoubleType,true), StructField(val51,DoubleType,true), StructField(val52,DoubleType,true), StructField(val53,DoubleType,true), StructField(val54,DoubleType,true), StructField(val55,DoubleType,true), StructField(val56,DoubleType,true))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//////////////////////////////////////////////////////\n",
    "// Classification sur SpamDataBase\n",
    "///////////////////////////////////////////////////\n",
    "import sys.process._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import collection.mutable\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "\n",
    "// Fonction qui produit le schéma du Dataframe sans la dernière colonne\n",
    "def genSpamFields(from: Int, to: Int) = for (i <- from until to)\n",
    "                         yield StructField(\"val\"+i.toString, DoubleType, true)\n",
    "// Génération des StructField successifs correspondant aux 57 premières colonnes\n",
    "val spamFields = genSpamFields(0, 57)\n",
    "// Construction du schéma complet du Dataframe (incluant la dernière colonne)\n",
    "val spamSchema = StructType(spamFields).add(\"label\", DoubleType, true)\n",
    "// Lecture des données et création du Dataframe\n",
    "val spamDF = spark.read.format(\"csv\").schema(spamSchema).load(\"data/spambase.data\").cache()\n",
    "\n",
    "val n: Int = 0\n",
    "val m:Int = 56\n",
    "\n",
    "val featureCols = spamDF.columns.slice(n, m)\n",
    "\n",
    "val exprs = featureCols.map(c => col(c).cast(\"double\"))\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val spamVector = assembler.transform(spamDF.select(exprs: _*)).select($\"features\")\n",
    "val kmeans = new KMeans().setK(5).setMaxIter(200).setSeed(1L)\n",
    "val modele = kmeans.fit(spamVector)\n",
    "\n",
    "val indices = modele.transform(spamVector)\n",
    "spamVector.select(\"features\").map(p => p.toString.filter(c => c != '[' & c != ']'& c != ')'& c != '(')).write.text(\"repvecteur\")\n",
    "Process(Seq(\"bash\",\"-c\",\"cat repvecteur/part* > spamvecteur\")).!\n",
    "\n",
    "indices.select(\"prediction\").map(p => p.toString.filter(c => c != '[' & c != ']'& c != ')'& c != '(')).write.text(\"repindice\")\n",
    "Process(Seq(\"bash\",\"-c\",\"cat repindice/part* > spamgroupes\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"rm -rf repvecteur;rm -rf repindice\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"paste --delimiters=',' spamvecteur spamgroupes > spam.Gnuplot\")).!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Pipeline pour Kmeans\n",
    "import sys.process._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "// Construction de DataFrame avec les données de Spambase\n",
    "def genSpamFields(from: Int, to: Int) = for (i <- from until to)\n",
    "                         yield StructField(\"val\"+i.toString, DoubleType, true)\n",
    "val spamFields = genSpamFields(0, 57)\n",
    "val spamSchema = StructType(spamFields).add(\"label\", DoubleType, true)\n",
    "val spamDF = spark.read.format(\"csv\").schema(spamSchema).load(\"data/spambase.data\")\n",
    "val colsEntree = {for (i <- 0 until 57) yield \"val\"+i.toString}.toArray\n",
    "val assembleur = new VectorAssembler().setInputCols(colsEntree).setOutputCol(\"features\")\n",
    "val spamDFA = assembleur.transform(spamDF).cache()\n",
    "// Partitionnement en données de train (80%) et données de test (20%)\n",
    "val spamSplits = spamDFA.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "// Construction d'un pipeline\n",
    "val scaler = new StandardScaler().setInputCol(\"features\")\n",
    "                                        .setOutputCol(\"scaledFeatures\")\n",
    "                                        .setWithStd(true)\n",
    "                                        .setWithMean(true)\n",
    "val kmeansNS = new KMeans().setFeaturesCol(scaler.getOutputCol)\n",
    "                                  .setPredictionCol(\"predictionNS\")\n",
    "                                  .setK(5)\n",
    "                                  .setMaxIter(200)\n",
    "                                  .setSeed(1L)\n",
    "val pipeline = new Pipeline().setStages(Array(scaler, kmeansNS))\n",
    "\n",
    "// Estimation des modèles du pipeline (sur données de train):\n",
    "//   le premier modèle (normalisation) est estimé en premier,\n",
    "//   ensuite il est utilisé comme transformer pour modifier les données\n",
    "//   et c'est le second modèle qui est estimé (clustering)\n",
    "//\n",
    "val modeleKMNS = pipeline.fit(spamSplits(0))\n",
    "\n",
    "// Application du modèle pour prédire les groupes des données de test\n",
    "//\n",
    "val indicesSpamTest = modeleKMNS.transform(spamSplits(1))\n",
    "                                       .select(\"scaledFeatures\",\"predictionNS\")\n",
    "\n",
    "indicesSpamTest.select(\"scaledFeatures\")\n",
    "                    .map(v => v.toString.filter(c => c != '[' & c != ']'))\n",
    "                    .write.text(\"data/vecteursTestSpam\")\n",
    "indicesSpamTest.select(\"predictionNS\")\n",
    "                      .map(p => p.toString.filter(c => c != '[' & c != ']'))\n",
    "                      .write.text(\"data/indicesTestSpam\")\n",
    "Process(Seq(\"bash\",\"-c\",\"cat data/vecteursTestSpam/part* > data/vecteurs\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"cat data/indicesTestSpam/part* > data/idgroupes\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"rm -rf data/vecteursTestSpam;rm -rf data/indicesTestSpam\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"paste --delimiters=',' data/vecteurs data/idgroupes > spam_Gnuplot.txt\")).!\n",
    "\n",
    "\n",
    "\n",
    "// Un pipeline peut être sauvegardé (avant estimation)\n",
    "pipeline.write.overwrite().save(\"spark-pipeline-normEtKMeans\")\n",
    "\n",
    "// Un modèle issu d'un pipeline (après estimation) peut être sauvegardé\n",
    "modeleKMNS.write.overwrite().save(\"spark-modele-clustering-spam\")\n",
    "\n",
    "// Un modèle sauvegardé peut être chargé pour être employé\n",
    "val memeModele = PipelineModel.load(\"spark-modele-clustering-spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[val0: double, val1: double ... 57 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamSplits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca = pca_db1df9d91d11\n",
       "resultat = [pcaFeatures: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[0.9667801236430948,0.032715960787922375]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// on fait une ACP pour avoir un K-means sur les composantes => reduction à 3 dimensions significatives\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "\n",
    "// Construction et application d'une nouvelle instance d'estimateur PCA\n",
    "val pca = new PCA().setInputCol(\"Features\").setOutputCol(\"pcaFeatures\")\n",
    "                          .setK(3).fit(spamSplits(1).select(\"Features\"))\n",
    "\n",
    "// Application du « transformateur » PCAModel résultant de l'estimation\n",
    "val resultat = pca.transform(spamSplits(1).select(\"Features\")).select(\"pcaFeatures\")\n",
    "\n",
    "// Les 3 plus grandes valeurs propres (exprimées en proportion de variance expliquée)\n",
    "pca.explainedVariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultat.select(\"pcaFeatures\").map(p => p.toString.filter(c => c != '[' & c != ']'))\n",
    "                      .write.text(\"data/PCASpam\")\n",
    "Process(Seq(\"bash\",\"-c\",\"cat data/PCASpam/part* > data/vecteursPCA\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"rm -rf data/PCASpam\")).!\n",
    "Process(Seq(\"bash\",\"-c\",\"paste --delimiters=',' data/vecteursPCA data/idgroupes > spam_PCA_Gnuplot.txt\")).!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
