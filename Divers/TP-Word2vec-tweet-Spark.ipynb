{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors, DenseVector, SparseVector}\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.util.KMeansDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg.{DenseVector=>BDV, SparseVector=>BSV, Vector=>BV}\n",
       "import org.apache.spark.mllib.linalg.{Vector=>SparkVector}\n",
       "toBreeze: (v: org.apache.spark.mllib.linalg.Vector)breeze.linalg.Vector[Double]\n",
       "fromBreeze: (bv: breeze.linalg.Vector[Double])org.apache.spark.mllib.linalg.Vector\n",
       "add: (v1: org.apache.spark.mllib.linalg.Vector, v2: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\n",
       "scalarMultiply: (a: Double, v: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => SparkVector}\n",
    "def toBreeze(v:SparkVector) = BV(v.toArray)\n",
    "def fromBreeze(bv:BV[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:SparkVector, v2:SparkVector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:SparkVector) = fromBreeze(a * toBreeze(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stopWords = Set(serious, latterly, down, side, moreover, please, ourselves, behind, for, find, further, mill, due, any, wherein, across, twenty, name, this, in, move, myself, have, your, off, once, are, is, his, why, too, among, everyone, show, empty, already, nobody, less, am, hence, system, than, four, fire, anyhow, three, whereby, himself, con, twelve, throughout, but, whether, below, co, mine, becomes, eleven, what, would, although, elsewhere, another, front, if, hereby, own, neither, bottom, up, etc, so, our, per, therein, must, beforehand, keep, do, all, him, had, somehow, re, onto, nor, every, herein, full, before, afterwards, somewhere, whither, else, namely, us, it, whereupon, two, thence, a, sometimes, became, thou...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Set(serious, latterly, down, side, moreover, please, ourselves, behind, for, find, further, mill, due, any, wherein, across, twenty, name, this, in, move, myself, have, your, off, once, are, is, his, why, too, among, everyone, show, empty, already, nobody, less, am, hence, system, than, four, fire, anyhow, three, whereby, himself, con, twelve, throughout, but, whether, below, co, mine, becomes, eleven, what, would, although, elsewhere, another, front, if, hereby, own, neither, bottom, up, etc, so, our, per, therein, must, beforehand, keep, do, all, him, had, somehow, re, onto, nor, every, herein, full, before, afterwards, somewhere, whither, else, namely, us, it, whereupon, two, thence, a, sometimes, became, though, within, as, because, well, meanwhile, has, she, yours, whose, yet, or, seems, describe, above, yourself, computer, herself, others, such, they, each, last, de, formerly, i, until, whatever, that, out, whenever, whereafter, amount, cannot, upon, to, become, sometime, least, now, toward, fifteen, hers, you, around, eg, most, here, these, was, six, there, found, something, nothing, nowhere, at, through, been, thick, often, go, someone, over, also, bill, can, on, being, same, how, whom, my, interest, after, everywhere, therefore, take, who, hundred, itself, everything, third, me, them, fill, besides, by, then, he, either, indeed, even, however, should, will, via, hasnt, few, back, much, again, while, their, anyway, fify, not, with, from, still, un, thru, whereas, next, nevertheless, give, inc, first, alone, both, sincere, ltd, could, thereupon, put, done, sixty, rather, ten, thus, anyone, whole, seem, ie, its, under, which, almost, an, five, whence, detail, be, noone, into, hereupon, where, get, her, themselves, always, were, enough, part, anything, hereafter, several, more, latter, anywhere, thereafter, between, wherever, amongst, none, ours, about, seeming, many, except, becoming, see, amoungst, call, against, made, during, thin, no, very, we, top, whoever, some, together, when, seemed, former, thereby, along, yourselves, towards, may, might, other, of, since, nine, mostly, forty, and, one, without, ever, couldnt, beyond, otherwise, beside, eight, cry, perhaps, the, cant, never, those, only)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// lire les stop words\n",
    "import scala.io.Source\n",
    "val stopWords = Source.fromFile(\"/home/user/Documents/RCP-216/data/stop_words\").getLines.toSet\n",
    "\n",
    "// transmettre les stop words aux noeuds de calcul\n",
    "val bStopWords = sc.broadcast(stopWords)\n",
    "\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"/home/user/Documents/RCP-216/data/w2vModel\")\n",
    "\n",
    "// obtenir une Map[String, Array[Float]] sérializable\n",
    "//   mapValues seul ne retourne pas une map sérializable (SI-7005)\n",
    "val vectors = w2vModel.getVectors.mapValues(vv => Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "\n",
    "// transmettre la map aux noeuds de calcul\n",
    "val bVectors = sc.broadcast(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectSize = 100\n",
       "sentences = /home/user/Documents/RCP-216/data/tweets MapPartitionsRDD[8] at textFile at <console>:58\n",
       "sent2vec = MapPartitionsRDD[12] at filter at <console>:78\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at filter at <console>:78"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// taille par défaut des vecteurs Word2Vec\n",
    "val vectSize = 100\n",
    "\n",
    "// lecture du fichier de tweets dans un RDD (item = ligne)\n",
    "val sentences = sc.textFile(\"/home/user/Documents/RCP-216/data/tweets\")\n",
    "\n",
    "// calcul des représentations Word2Vec des tweets\n",
    "val sent2vec = sentences.filter(sentence => sentence.length >= 1)\n",
    "    .map(sentence => sentence.toLowerCase.split(\"\\\\W+\"))\n",
    "    .map(wordSeq => {\n",
    "        var vSum = Vectors.zeros(vectSize)\n",
    "        var vNb = 0\n",
    "        wordSeq.foreach { word =>\n",
    "            if(!(bStopWords.value)(word) & (word.length >= 2)) {\n",
    "                bVectors.value.get(word).foreach { v =>\n",
    "                    vSum = add(v, vSum)\n",
    "                    vNb += 1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if (vNb != 0) {\n",
    "            vSum = scalarMultiply(1.0 / vNb, vSum)\n",
    "        }\n",
    "        vSum\n",
    "    }).filter(vec => Vectors.norm(vec, 1.0) > 0.0).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "739"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.027988523244857788,-0.07431866228580475,0.09145046770572662,-0.01157666090875864,0.03929232805967331,-0.05890924111008644,0.08137806504964828,0.12244899570941925,0.026949968189001083,0.10132735967636108,0.029005125164985657,0.021411007270216942,-0.004389403387904167,-0.041520968079566956,0.02486402727663517,-0.04098132997751236,0.05608462542295456,-0.045475587248802185,-0.13348358869552612,-0.04931315779685974,0.008751633577048779,0.0360114760696888,-0.0059855664148926735,0.02956448681652546,0.08999983221292496,-0.13633666932582855,-0.05564919114112854,-0.0076880375854671,-0.04098349064588547,0.03392435610294342,0.20209497213363647,0.050972163677215576,-0.04521965980529785,0.0523226223886013,0.09931804984807968,-0.002539577428251505,7.794810517225415E-5,-0.06215415894985199,-0.13437716662883759,0.08244974911212921,-0.09550885111093521,-0.04567639157176018,0.06954824179410934,0.02887226641178131,0.06212705373764038,-0.027039499953389168,-0.030935458838939667,0.08454225212335587,-0.07133147865533829,0.035270001739263535,0.08300800621509552,-0.1998845487833023,-0.08018776774406433,-0.06998732686042786,0.1202029287815094,-0.0050324746407568455,-0.14934572577476501,-0.04219688102602959,0.05759391561150551,-0.05430854856967926,0.09008777141571045,0.03772690147161484,-0.05574829503893852,0.05453091487288475,-0.12100667506456375,-0.07600320130586624,-0.11300190538167953,0.009957455098628998,0.018486062064766884,-0.018400315195322037,0.015907417982816696,0.04891902580857277,-0.09369911998510361,0.03777400404214859,-0.07512087374925613,0.10543429851531982,0.06334846466779709,0.09136684983968735,-0.007695410400629044,-0.07017897814512253,-1.501155347796157E-4,0.006943205837160349,-0.11013315618038177,-0.03523114323616028,0.019961630925536156,0.06619110703468323,0.06772404909133911,0.029875753447413445,-0.11104890704154968,0.02659784071147442,-0.026555996388196945,-0.04203786700963974,-0.07580332458019257,0.032733432948589325,0.009125785902142525,0.06218285858631134,0.016012409701943398,0.06372857093811035,-0.08021727204322815,-0.06459298729896545], [-0.1365070343017578,-0.14906281232833862,0.39309272170066833,-0.013833335600793362,0.30419066548347473,-0.03517115116119385,0.01976456679403782,-0.01334411557763815,0.4497770071029663,0.10581625998020172,-0.2356569916009903,-0.09287767857313156,-0.1415044069290161,0.15903587639331818,-0.17917750775814056,-0.2456539124250412,0.07598385214805603,0.4254184365272522,-0.27263835072517395,-0.23669666051864624,-0.04780809208750725,-0.18694566190242767,0.08121707290410995,-0.13863374292850494,0.3532973527908325,-0.36334213614463806,-0.1732996255159378,-0.3129798173904419,-0.2980506718158722,0.23395155370235443,0.32112449407577515,0.022843746468424797,0.11316071450710297,0.08550429344177246,0.10486443340778351,0.3420502543449402,-0.06793404370546341,-0.16425994038581848,-0.0973147600889206,-0.004417809192091227,-0.3879908323287964,0.14311063289642334,0.272991806268692,0.06757650524377823,0.236961230635643,-0.016038209199905396,0.21575434505939484,-0.08593109995126724,-0.13186977803707123,0.1158967912197113,0.1342690885066986,-0.16157662868499756,0.19690380990505219,0.15257538855075836,0.11885375529527664,-0.03878413885831833,-0.15650281310081482,0.018333693966269493,0.02641383185982704,-0.29774928092956543,0.1679210662841797,0.3191642761230469,-0.18715225160121918,0.00878765806555748,-0.37011516094207764,-0.2367992103099823,0.13908374309539795,-0.3786006271839142,0.004917545709758997,1.715175312710926E-4,0.05350220575928688,0.06406733393669128,-0.3197607696056366,-0.27830612659454346,-0.0014414320467039943,0.0305640809237957,-0.2251419723033905,0.1969759613275528,-0.07916028052568436,-0.2589464485645294,-0.1416073590517044,-0.01460847444832325,-0.2960170805454254,-0.04464581608772278,-0.009269675239920616,0.26420900225639343,0.1759398877620697,-0.49993059039115906,-0.023272277787327766,0.3832499384880066,0.01451894361525774,0.3843090534210205,0.3775249421596527,-0.0777752697467804,0.13806380331516266,0.21490544080734253,0.3370620608329773,-0.05085785314440727,0.42840033769607544,0.19063976407051086]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 144:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nbClusters = 20\n",
       "nbIterations = 200\n",
       "clustering = org.apache.spark.mllib.clustering.KMeansModel@320b8539\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@320b8539"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbClusters = 20\n",
    "val nbIterations = 200\n",
    "val clustering = KMeans.train(sent2vec, nbClusters, nbIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " weather (0,819), transmissions (0,635), updates (0,626), servicing (0,605), geostationary (0,591),\n",
      " thank (0,889), yes (0,843), sorry (0,787), tidak (0,744), vai (0,743),\n",
      " right (0,931), left (0,872), handed (0,701), frac (0,653), vec (0,614),\n",
      " day (0,854), week (0,742), days (0,721), year (0,719), month (0,691),\n",
      " night (1,000), saturday (0,732), morning (0,674), grinch (0,621), monday (0,606),\n",
      " oh (0,968), ch (0,676), cooh (0,633), ik (0,622), tib (0,592),\n",
      " joanie (0,802), peck (0,801), fisher (0,796), heather (0,793), bucks (0,792),\n",
      " teen (0,752), tonight (0,751), bullshit (0,745), bowie (0,731), ziggy (0,725),\n",
      " https (0,917), edu (0,793), php (0,791), howstuffworks (0,789), download (0,784),\n",
      " wish (0,839), want (0,805), feel (0,801), despise (0,790), presume (0,787),\n",
      " hackney (0,686), webcam (0,677), yazoo (0,668), listings (0,655), bemani (0,654),\n",
      " https (1,000), edu (0,864), geocities (0,862), adb (0,821), ibiblio (0,813),\n",
      " unrestricted (0,753), incentives (0,745), transfers (0,724), upload (0,723), interactively (0,722),\n",
      " silly (0,757), guesses (0,718), caddies (0,708), sorceress (0,708), paladin (0,705),\n",
      " gave (0,931), took (0,563), returned (0,545), chose (0,542), gives (0,515),\n",
      " best (1,000), award (0,573), awards (0,559), bafta (0,553), grammy (0,544),\n",
      " want (0,831), yourself (0,809), tell (0,809), wouldn (0,808), ask (0,802),\n",
      " david (0,804), friend (0,775), zappa (0,766), cronenberg (0,748), murdock (0,732),\n",
      " everybody (0,807), hang (0,776), myself (0,765), plissken (0,762), cried (0,759),\n",
      " rt (0,757), wo (0,714), gl (0,675), simpson (0,672), carey (0,665),\n"
     ]
    }
   ],
   "source": [
    "clustering.clusterCenters.foreach(clusterCenter => {\n",
    "    w2vModel.findSynonyms(clusterCenter,5).foreach(synonym => print(\" %s (%5.3f),\"\n",
    "            .format(synonym._1, synonym._2)))\n",
    "    println()\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
